{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/practice_cnn.png\" width=\"800\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. 컨볼루셔널 뉴럴 네트워크(CNN)\n",
    "    (1) [이론] \n",
    "        - 다층 퍼셉트론의 경우 n차원 정보를 1차원으로 변경시킨 후 입력해야 하기 때문에 고유 정보를 손실한다는 단점이 있음\n",
    "        - 또한 (예를 들어 이미지의 경우) 1차원에선 픽셀 하나하나의 영향이 매우 크게 나타남 -> 상당히 많은 변수를 통제해야 한다는 단점\n",
    "        - CNN은 사물의 생김새 정보 그 자체로 사물을 학습하고 구별\n",
    "        \n",
    "        (i) CNN은 어떻게 특징을 찾아내는가\n",
    "            - CNN은 고유 이미지를 형변환없이 그대로 입력데이터로 처리\n",
    "            - 이 과정에서 필터를 이용, 필터를 이미지에 통과시킴으로써(스트라이드) 이미지 구별\n",
    "            - 필터는 특징을 추출하기 위한 상자같은 것으로, 필터와 이미지가 겹치는 부분을 '수용영역'\n",
    "            - 필터 안에는 특정한 숫자 값이 있으며, 필터와 이미지 영역의 겹치는 부분마다 곱셈이 이루어짐 -> 최종값이 크다(적다) <=> 필터와 겹치는 부분이 많다(적다)\n",
    "            - CNN 모델 안에는 각 특징의 개수만큼 여러개의 필터가 존재\n",
    "            - 스트라이드를 통해 얻어진 행렬을 '피처맵'이라고 하고, 피처맵을 활성화함수에 넣어 구한 행렬을 '액티베이션맵'\n",
    "            - 액티베이션맵은 '풀링 레이어'를 거치면서 계산의 용이성과 과대적합 방지를 위해 보다 작은 값으로 변환됨\n",
    "        \n",
    "        (ii) 제로 패딩\n",
    "            - 0으로 입력행렬의 테두리를 감싸는 기술\n",
    "            - CONV(필터가 존재하는 레이어)에 의한 정보 손실 방지 + CNN이 테두리 정보를 활용할 수 있게 됨\n",
    "            \n",
    "        (iii) CNN으로 컬러이미지 분류하기\n",
    "            - 흑백이미지의 텐서는 (28 x 28 기준) (n, 28, 28, 1)이고 컬러이미지는 빨강, 녹색 , 파랑을 사용하므로 (n, 28, 28, 3) ; n은 데이터 수\n",
    "            - 컬러이미지는 0부터 255까지의 값을 지닌 빨강, 녹색, 파랑 레이어가 겹쳐보이는 색상 -> 겹쳐진 레이어의 개수를 '깊이'라고 부름\n",
    "            - 컬리이미지를 빨강, 녹색, 파랑으로 분리 후 각각의 필터를 사용 후 만든 3개의 피처맵을 합쳐서 편향값까지 더해 하나의 피처맵을 나타냄\n",
    "            - 한개의 필터는 하나의 피처맵을 출려하므로 10개의 필터가 한 CONV에 있으면 다음 레이어에 입력되는 깊이는 10\n",
    "            \n",
    "        (iv) CNN 모델 학습 이해하기\n",
    "            - 이미지 -> CONV1 -> CONV2 -> POLL -> FLATTEN -> FC -> 옵티마이저\n",
    "            - 실습 시 이미지 참고\n",
    "            \n",
    "        (v) 파라미터 최적화\n",
    "            - CNN은 CONV 와 FC에 파라미터가 존재, CONV는 특징을 추출하는 레이어고 FC는 추출된 특징을 가지고 분류를 수행하는 레이어\n",
    "            - 필터의 역할은 최적화 과정을 통해 모델이 스스로 찾아서 부여\n",
    "            - CNN은 보통 경사하강법을 사용해 모델 최적화\n",
    "\"\"\"\n",
    "\n",
    "#(2) [실습] CNN\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/practice_cnn.png\", width=800, height=200)\n",
    "\n",
    "# 입력(28 x 28의 흑백 이미지) -> CONV1(28x28 필터 16개를 이용, 16개의 피처맵 생성) -> POOL(14x14 크기의 피처맵으로 축소) -> CONV2(14x14 필터 32개 이용, 32개 피처맵 생성)\n",
    "# -> POOL(14x14사이즈의 피처맵 32개로 축소) ->  FLATTEN -> FC -> 옵티마이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#데이터획득\n",
    "(x_train, y_train) , (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ed761b1630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "x_val = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]\n",
    "\n",
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 구조 변경하기\n",
    "# 28 x 28 흑백 이미지이므로 데이터의 형태를 (28,28,1)로 맞춰줌(흑백이미지이기 때문)\n",
    "\n",
    "import numpy as np\n",
    "x_train = np.reshape(x_train, (50000,28,28,1))\n",
    "x_val = np.reshape(x_val, (10000,28,28,1))\n",
    "x_test = np.reshape(x_test, (10000, 28, 28, 1))\n",
    "\n",
    "#데이터 정규화\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_val /= gray_scale\n",
    "\n",
    "#원 핫 인코딩\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 텐서플로로 구현하기\n",
    "\n",
    "#입력 데이터 포맷\n",
    "x = tf.placeholder(tf.float32, shape = [None, 28, 28, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])\n",
    "\n",
    "#파라미터 초기값 설정\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) # 0에서부터 거리가 아주 먼 값으로 설정되지 않게 해줌(시그모이드의 경우 입력값이 매우 크거나 작으면 그 미분값이 0과 가까워져서 경사하가법으로 파라미터를 변경하기 어려워짐)\n",
    "                                                     # truncated_normal은 꼬리를 자른 정규분포로 옵션을 통해 mean, stddev, dtype조절 가능\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = 'SAME') # padding = 'SAME' 옵션은 레이어에 입력된 피처맵의 사이즈와 동일하게 피처맵을 출력하도록 설정 ; 패딩을 하지 않을 경우 VALID\n",
    "    # tf.nn.conv2d(input, filters, strides, padding), input은 입력 이미지 집합으로 4차원 데이터여야함 : [batch, in_height, in_width, on_channels], 각각 데이터개수, 높이, 너비, 이미지의 컬러채널(흑백 1, 칼라 3)\n",
    "    # filters 역시 4차원 데이터로 [filter_height, filter_width, in_channels, out_channels], 각각 필터 높이, 너비, 필터에 입력되는 이미지의 컬러채널, 필터의 종류\n",
    "    # stride는 인수의 길이가 4인 벡터, 첫번째와 네번째 수는 항상 1이어야 함, 두번째와 네번째는 가로방향과 세로방향으로 스캐닝을 할 때 건너 뛰는 데이터 숫자로 건너뛰지 않고 모두 필터링 하는 경우엔 strides = [1,1,1,1]\n",
    "    \n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    #tf.nn.max_pool(input, ksize, strides, padding, data_format = None , name = None)\n",
    "\n",
    "W_conv1 = weight_variable([5,5,1,16]) #첫번째 CONV 레이어는 5x5 필터사이즈의 16개 필터\n",
    "b_conv1 = bias_variable([16])\n",
    "\n",
    "#활성화 함수로 ReLU 사용\n",
    "h_conv1 = tf.nn.relu(conv2d(x,W_conv1) + b_conv1)\n",
    "\n",
    "#풀링레이어 적용, 액티베이션 맵의 크기를 줄여줌\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#두번째 CONV 레이어, 총 32개의 필터\n",
    "W_conv2 = weight_variable([5,5,16,32])\n",
    "b_conv2 = bias_variable([32])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FC(Fully Connected Layer) - 첫번째 FC(128개의 노드)\n",
    "W_fc1 = weight_variable([7*7*32, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*32])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#두번째 FC(10개의 노드)\n",
    "W_fc2 = weight_variable([128,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크로스 엔트로피 설정\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아답 옵티마이저를 사용해 모델 최적화, 학습률 = 0.001\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정확도 구학\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1) , tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train acc :0.138\n",
      "step 10: train acc :0.68\n",
      "step 20: train acc :0.802\n",
      "step 30: train acc :0.868\n",
      "step 40: train acc :0.876\n",
      "step 50: train acc :0.908\n",
      "step 60: train acc :0.926\n",
      "step 70: train acc :0.946\n",
      "step 80: train acc :0.946\n",
      "step 90: train acc :0.936\n",
      "validation accuracy : 0.1064\n",
      "step 0: train acc :0.952\n",
      "step 10: train acc :0.946\n",
      "step 20: train acc :0.952\n",
      "step 30: train acc :0.954\n",
      "step 40: train acc :0.946\n",
      "step 50: train acc :0.962\n",
      "step 60: train acc :0.972\n",
      "step 70: train acc :0.96\n",
      "step 80: train acc :0.974\n",
      "step 90: train acc :0.972\n",
      "validation accuracy : 0.1064\n",
      "step 0: train acc :0.966\n",
      "step 10: train acc :0.972\n",
      "step 20: train acc :0.962\n",
      "step 30: train acc :0.962\n",
      "step 40: train acc :0.97\n",
      "step 50: train acc :0.978\n",
      "step 60: train acc :0.978\n",
      "step 70: train acc :0.972\n",
      "step 80: train acc :0.978\n",
      "step 90: train acc :0.978\n",
      "validation accuracy : 0.1064\n",
      "test accuracy :0.9744\n"
     ]
    }
   ],
   "source": [
    "#학습 및 테스트\n",
    "\n",
    "#초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#하이퍼 파라미터 세팅\n",
    "epoch_cnt = 3\n",
    "batch_size = 500\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "#학습시작\n",
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed(777)\n",
    "    \n",
    "    sess.run(init)\n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            if i % 10 == 0:\n",
    "                train_acc = accuracy.eval(feed_dict = {x : x_train[start:end], y_ : y_train[start:end]})\n",
    "                print('step '+str(i) + ': train acc :'+str(train_acc))\n",
    "            \n",
    "            train_step.run(feed_dict = {x:x_train[start:end], y_:y_train[start:end]})\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "    \n",
    "    #모델 검증\n",
    "        val_accuracy = accuracy.eval(feed_dict={x:x_val , y_:y_val})\n",
    "        print(\"validation accuracy : \" +str(val_accuracy))\n",
    "    \n",
    "    test_accuracy = accuracy.eval(feed_dict = {x:x_test, y_:y_test})\n",
    "    print(\"test accuracy :\" + str(test_accuracy))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3번의 주기 만으로 97%의 정확도를 보임\n",
    "# validation accuracy 에러는 잘 모르겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 3), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(1, 3), dtype=float32)\n",
      "weights\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. 순환신경망(RNN)\n",
    "    (1) [이론] RNN\n",
    "        - 순차적인 데이터를 입력받아 결괏값을 도출하는데 사용하는 딥러닝 모델 : 이전 입력값들을 고려해서 현재 입력값의 출력값을 결정\n",
    "        - 자연어 처리에 많이 사용\n",
    "        \n",
    "        (i) RNN 구조\n",
    "            - 레이어에 해당하는 개념으로 '셀'이 존재, 현재 셀의 입력값과 과거 셀의 상태값을 사용해 현재 셀의 상태값 계산 -> 반복\n",
    "            - h_t = tanh(w_xh * x_t + w_hh* h_t-1 + b)\n",
    "        \n",
    "        (ii) RNN으로 문장의 감정 분석하기\n",
    "            - 문장의 감정은 처음부터 끝까지 단어를 읽은 후 가능 -> RNN은 이와 같은 문제에 적합한 구조를 가지고 있음\n",
    "            - 손실함수를 통해 최종 상태값에 소프트매스를 취한 값과 실제값 간의 거리를 최소화함으로써 모델 학습 가능\n",
    "            \n",
    "    (2) [이론] LSTM\n",
    "        - RNN 셀 구조 중 하나\n",
    "        - 경사하강법으로 RNN 학습을 진행할 때 gradient vanishing 또는 gradient exploding으로 인해 모델이 최적화되지 않음 -> 이를 보완하고자 LSTM 셀이 등장\n",
    "        \n",
    "        (i) RNN 학습\n",
    "            - RNN은 경사하강법 기반 알고리즘으로 모델 최적화 : W = W - learning_rate * ∂E/∂W\n",
    "            - 입력값이 여러개일 경우 chain rule에 따라 계산이 매우 복잡해짐\n",
    "            - gradient vanishing : 연속해서 곱해지는 모든 미분값들이 1보다 작은 값일 때 멀리있는 정보가 현재 셀에 거의 영향을 미치지 못하는 상황\n",
    "            - gradient exploding : '' 1보다 큰 값일 때 파라미터가 급변하여 최적의 파라미터 W를 찾지 못하게 되는 현상\n",
    "        \n",
    "        (ii) LSTM의 등장\n",
    "            - '메모리셀'이 추가, 불필요한 정보는 잊고, 필요한 정보는 저장하는 역할 수행\n",
    "        \n",
    "        (iii) LSTM의 구조 및 메커니즘\n",
    "            1) 기억손실 메커니즘 : 메모리셀이 기억하는 과거의 정보에 시그모이드 함수가 곱해져서 1일 경우 과거 메모리셀을 그대로 유지, 1보다 작을 경우 그에 해당하는 값만큼의 기억만 유지\n",
    "            2) 입력 메커니즘 : 현재 입려값을 메모리셀에 저장하는 메커니즘\n",
    "            3) 출력 메커니즘 : 현재 상태값을 출력할 때 메모리셀의 정보 이용\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "#.(3) [실습] RNN 기초\n",
    "import numpy as np\n",
    "\n",
    "#셀이 하나인 경우 - 출력값(output)과 상태값(sate)가 같다 + 가중치 확인해보기\n",
    "#기본구조는 입력값(x), 출력값(output), 상태값(state), 가중치(W), 편향값(b), 활성화함수(tanh)로 구성\n",
    "import tensorflow as tf\n",
    "inputs = np.array([[[3,4,5]]])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype = tf.float32)\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=3)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell = rnn_cell, dtype = tf.float32, inputs = tf_inputs)\n",
    "\n",
    "variables_names = [v.name for v in tf.trainable_variables()]\n",
    "print(outputs)\n",
    "print(state)\n",
    "print('weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rnn/basic_rnn_cell/kernel:0', 'rnn/basic_rnn_cell/bias:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.59238183,  0.36334312,  0.75173855],\n",
      "       [-0.49057457,  0.2832818 , -0.76840913],\n",
      "       [ 0.37146485,  0.15716732,  0.3830546 ],\n",
      "       [-0.60262036,  0.7976357 ,  0.4965744 ],\n",
      "       [ 0.38772213,  0.6255324 , -0.01987052],\n",
      "       [ 0.32896936, -0.4309209 ,  0.24104548]], dtype=float32), array([0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values = sess.run(variables_names)\n",
    "    print(values) #  지금 보여지는 것은 weight값을 나타냄, x -> h 로 갈때 w_hx는 3 x 3, h -> y로 갈 때 w_hy는 3 x 3, 그래서 총 6행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(6, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n",
      "output values\n",
      "[[[-0.95468026  0.99514276  0.7993655 ]]]\n",
      "\n",
      "state values\n",
      "[[-0.95468026  0.99514276  0.7993655 ]]\n",
      "\n",
      "weight\n",
      "rnn/basic_rnn_cell/kernel:0 [[-0.59238183  0.36334312  0.75173855]\n",
      " [-0.49057457  0.2832818  -0.76840913]\n",
      " [ 0.37146485  0.15716732  0.3830546 ]\n",
      " [-0.60262036  0.7976357   0.4965744 ]\n",
      " [ 0.38772213  0.6255324  -0.01987052]\n",
      " [ 0.32896936 -0.4309209   0.24104548]]\n",
      "rnn/basic_rnn_cell/bias:0 [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(v)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([outputs, state])\n",
    "    print('output values')\n",
    "    print(output_run)\n",
    "    print('\\nstate values')\n",
    "    print(state_run)\n",
    "    print('\\nweight')\n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/transpose_1:0\", shape=(2, 4, 3), dtype=float32)\n",
      "Tensor(\"rnn/while/Exit_3:0\", shape=(2, 3), dtype=float32)\n",
      "weight :\n",
      "\n",
      "<tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(7, 3) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(3,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "#위의 출력값을 통해 다음을 확인((inputs = np.array([[1,2]])인 경우를 말함)\n",
    "\n",
    "# RNN 셀이 한개일 경우 출력값과 상태값이 동일\n",
    "# output values\n",
    "# [[[-0.9314169   0.75578666 -0.6819246 ]]]\n",
    "\n",
    "# state values -> 대신 state value는 1x1x3이 아닌 1 x 3으로 나옴\n",
    "# [[-0.9314169   0.75578666 -0.6819246 ]]\n",
    "\n",
    "\n",
    "#입력값이 1*2 행렬이고 RNN이 1*3 행렬일 경우 W는 총 5개의 행을 갖게 됨\n",
    "# weight\n",
    "# [[-0.62831575  0.38538355  0.79733914]\n",
    "#  [-0.5203329   0.30046564 -0.8150209 ]\n",
    "#  [ 0.39399797  0.16670114  0.4062907 ]\n",
    "#  [-0.6391754   0.8460203   0.5266966 ]\n",
    "#  [ 0.41124135  0.66347724 -0.0210759 ]]\n",
    "\n",
    "#입력값이 1*2이고 RNN의 상태값이 1*3의 행렬일 경우 편향값은 총 3개가 필요\n",
    "#rnn/basic_rnn_cell/bias:0 [0. 0. 0.]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 텐서플로로 단어 품사 구분하기\n",
    "\n",
    "#I    [1,0,0,0]\n",
    "#work [0,1,0,0]\n",
    "#at   [0,0,1,0]\n",
    "#google[0,0,0,1]\n",
    "\n",
    "#I work at google  [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "#I google at work  [[1,0,0,0],[0,0,0,1],[0,0,1,0],[0,1,0,0]]\n",
    "\n",
    "inputs = np.array([\n",
    "    [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],\n",
    "    [[1,0,0,0],[0,0,0,1],[0,0,1,0],[0,1,0,0]]\n",
    "]) #input은 2 x 4 x 4\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "tf_inputs = tf.constant(inputs, dtype=tf.float32)\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=3) #output은 2 x 4 x 3으로 출력; (a,b,c)에서 c만 c'로 바꾸는 것\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=rnn_cell, dtype=tf.float32, inputs=tf_inputs)\n",
    "variables_names = [v.name for v in tf.trainable_variables()] # W가 저장된 주소 받기, W는 x가 h로 갈때 4 x 3 + h가 y로 갈때 3 x 3이므로 7 x 3\n",
    "\n",
    "print(outputs)\n",
    "print(state)\n",
    "print('weight :\\n')\n",
    "for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output values\n",
      "[[[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.20793891  0.24406303 -0.75278705]\n",
      "  [-0.06346128 -0.52844936  0.68356085]\n",
      "  [-0.36491966  0.8857268  -0.02324398]]\n",
      "\n",
      " [[-0.50944704  0.33166462  0.6126557 ]\n",
      "  [-0.30707452  0.62735885  0.21719742]\n",
      "  [ 0.5043804  -0.14038289  0.3744523 ]\n",
      "  [-0.11641283  0.70696247 -0.7512605 ]]]\n",
      "\n",
      "state value\n",
      "[[-0.36491966  0.8857268  -0.02324398]\n",
      " [-0.11641283  0.70696247 -0.7512605 ]]\n",
      "weights\n",
      "rnn/basic_rnn_cell/kernel:0 [[-0.56198275  0.34469748  0.7131618 ]\n",
      " [-0.4653999   0.2687447  -0.7289769 ]\n",
      " [ 0.35240245  0.14910203  0.36339748]\n",
      " [-0.57169586  0.7567036   0.47109187]\n",
      " [ 0.3678255   0.5934322  -0.01885086]\n",
      " [ 0.31208777 -0.40880746  0.22867584]\n",
      " [ 0.5521256   0.682691   -0.5481483 ]]\n",
      "rnn/basic_rnn_cell/bias:0 [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_run, state_run = sess.run([outputs, state])\n",
    "    print('output values')\n",
    "    print(output_run)\n",
    "    print('\\nstate value')\n",
    "    print(state_run)\n",
    "    print('weights')\n",
    "    values = sess.run(variables_names)\n",
    "    \n",
    "    for k,v in zip(variables_names, values):\n",
    "        print(k,v)\n",
    "        \n",
    "# state는 2 x 4 x 4-> 2 x 4 x 3 이 될 때 2 x 3이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output values\n",
      "[[[ 0.08924279]\n",
      "  [ 0.01773031]\n",
      "  [-0.03008365]\n",
      "  [ 0.02391436]]\n",
      "\n",
      " [[ 0.08924279]\n",
      "  [ 0.06824919]\n",
      "  [ 0.04068697]\n",
      "  [-0.00590078]]]\n",
      "\n",
      "memory cell value\n",
      "[[ 0.07265329]\n",
      " [-0.01204549]]\n",
      "\n",
      "hidden state value\n",
      "[[ 0.02391436]\n",
      " [-0.00590078]]\n"
     ]
    }
   ],
   "source": [
    "#4.[실습] LSTM 기초 - 출력값, 상태값, 메모리셀 값 출력하기\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "tf_inputs = tf.constant(inputs, dtype = tf.float32) #input은 2 x 4 x 4\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=1) #outputdms 2 x 4 x 1\n",
    "\n",
    "outputs, state = tf.nn.dynamic_rnn(cell = lstm_cell, dtype = tf.float32, inputs = tf_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    _output, _state = sess.run([outputs, state])\n",
    "    print('output values')\n",
    "    print(_output)\n",
    "    print('\\nmemory cell value') # 아웃풋에서 가운데 값 뺸 것 -> 2 x 1\n",
    "    print(_state.c)\n",
    "    print('\\nhidden state value') # 2 x 1\n",
    "    print(_state.h)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.[실습] LSTM - 지문을 읽고 주제 분류하기\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "paragraph_dict_list = [\n",
    "         {'paragraph': 'dishplace is located in sunnyvale downtown there is parking around the area but it can be difficult to find during peak business hours my sisters and i came to this place for dinner on a weekday they were really busy so i highly recommended making reservations unless you have the patience to wait', 'category': 'food'},\n",
    "         {'paragraph': 'service can be slower during busy hours but our waiter was courteous and help gave some great entree recommendations', 'category': 'food'},\n",
    "         {'paragraph': 'portions are huge both french toast and their various omelettes are really good their french toast is probably 1.5x more than other brunch places great place to visit if you are hungry and dont want to wait 1 hour for a table', 'category': 'food'},\n",
    "         {'paragraph': 'we started with apps going the chicken and waffle slides and chicken nachos the sliders were amazing and the nachos were good too maybe by themselves the nachos would have scored better but after those sliders they were up against some tough competition', 'category': 'food'},\n",
    "         {'paragraph': 'the biscuits and gravy was too salty two people in my group had the gravy and all thought it was too salty my hubby ordered a side of double egg and it was served on two small plates who serves eggs to one person on separate plates we commented on that when it was delivered and even the server laughed and said she doesnt know why the kitchen does that presentation of food is important and they really missed on this one', 'category': 'food'},\n",
    "         {'paragraph': 'the garlic fries were a great starter (and a happy hour special) the pancakes looked and tasted great and were a fairly generous portion', 'category': 'food'},\n",
    "         {'paragraph': 'our meal was excellent i had the pasta ai formaggi which was so rich i didnt dare eat it all although i certainly wanted to excellent flavors with a great texture contrast between the soft pasta and the crisp bread crumbs too much sauce for me but a wonderful dish', 'category': 'food'},\n",
    "         {'paragraph': 'what i enjoy most about palo alto is so many restaurants have dog-friendly seating outside i had bookmarked italico from when they first opened about a 1.5 years ago and was jonesing for some pasta so time to finally knock that bookmark off', 'category': 'food'},\n",
    "         {'paragraph': 'the drinks came out fairly quickly a good two to three minutes after the orders were taken i expected my iced tea to taste a bit more sweet but this was straight up green tea with ice in it not to complain of course but i was pleasantly surprised', 'category': 'food'},\n",
    "         {'paragraph': 'despite the not so good burger the service was so slow the restaurant wasnt even half full and they took very long from the moment we got seated to the time we left it was almost 2 hours we thought that it would be quick since we ordered as soon as we sat down my coworkers did seem to enjoy their beef burgers for those who eat beef however i will not be returning it is too expensive and extremely slow service', 'category': 'food'},\n",
    "    \n",
    "         {'paragraph': 'the four reigning major champions simona halep caroline wozniacki angelique kerber and defending us open champion sloane stephens could make a case for being the quartet most likely to succeed especially as all but stephens has also enjoyed the no1 ranking within the last 14 months as they prepare for their gruelling new york campaigns they currently hold the top four places in the ranks', 'category': 'sports'},\n",
    "         {'paragraph': 'the briton was seeded nn7 here last year before a slump in form and confidence took her down to no46 after five first-round losses but there have been signs of a turnaround including a victory over a sub-par serena williams in san jose plus wins against jelena ostapenko and victoria azarenka in montreal. konta pulled out of new haven this week with illness but will hope for good things where she first scored wins in a major before her big breakthroughs to the semis in australia and wimbledon', 'category': 'sports'},\n",
    "         {'paragraph': 'stephens surged her way back from injury in stunning style to win her first major here last year—and ranked just no83 she has since proved what a big time player she is winning the miami title via four fellow major champions then reaching the final at the french open back on north american hard courts she ran to the final in montreal only just edged out by halep she has also avoided many of the big names in her quarter—except for wild card azarenka as a possible in the third round', 'category': 'sports'},\n",
    "         {'paragraph': 'when it came to england chances in the world cup it would be fair to say that most fans had never been more pessimistic than they were this year after enduring years of truly dismal performances at major tournaments – culminating in the 2014 event where they failed to win any of their three group games and finished in bottom spot those results led to the resignation of manager roy hodgson', 'category': 'sports'},\n",
    "         {'paragraph': 'the team that eliminated russia – croatia – also improved enormously during the tournament before it began their odds were 33/1 but they played with real flair and star players like luka modric ivan rakitic and ivan perisic showed their quality on the world stage having displayed their potential by winning all three of their group stage games croatia went on to face difficult tests like the semi-final against england', 'category': 'sports'},\n",
    "         {'paragraph': 'the perseyside outfit finished in fourth place in the premier league table and without a trophy last term after having reached the champions league final before losing to real madrid', 'category': 'sports'},\n",
    "         {'paragraph': 'liverpool fc will return to premier league action on saturday lunchtime when they travel to leicester city in the top flight as they look to make it four wins in a row in the league', 'category': 'sports'},\n",
    "         {'paragraph': 'alisson signed for liverpool fc from as roma this summer and the brazilian goalkeeper has helped the reds to keep three clean sheets in their first three premier league games', 'category': 'sports'},\n",
    "         {'paragraph': 'but the rankings during that run-in to new york hid some very different undercurrents for murray had struggled with a hip injury since the clay swing and had not played a match since losing his quarter-final at wimbledon and he would pull out of the us open just two days before the tournament began—too late however to promote nederer to the no2 seeding', 'category': 'sports'},\n",
    "         {'paragraph': 'then came the oh-so-familiar djokovic-nadal no-quarter-given battle for dominance in the third set there were exhilarating rallies with both chasing to the net both retrieving what looked like winning shots nadal more than once pulled off a reverse smash and had his chance to seal the tie-break but it was djokovic serving at 10-9 who dragged one decisive error from nadal for a two-sets lead', 'category': 'sports'}\n",
    "]\n",
    "df = pd.DataFrame(paragraph_dict_list)\n",
    "df = df[['paragraph', 'category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dishplace is located in sunnyvale downtown the...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service can be slower during busy hours but ou...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portions are huge both french toast and their ...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we started with apps going the chicken and waf...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the biscuits and gravy was too salty two peopl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph category\n",
       "0  dishplace is located in sunnyvale downtown the...     food\n",
       "1  service can be slower during busy hours but ou...     food\n",
       "2  portions are huge both french toast and their ...     food\n",
       "3  we started with apps going the chicken and waf...     food\n",
       "4  the biscuits and gravy was too salty two peopl...     food"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #주제가 food인 지문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the perseyside outfit finished in fourth place...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>liverpool fc will return to premier league act...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alisson signed for liverpool fc from as roma t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>but the rankings during that run-in to new yor...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>then came the oh-so-familiar djokovic-nadal no...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            paragraph category\n",
       "15  the perseyside outfit finished in fourth place...   sports\n",
       "16  liverpool fc will return to premier league act...   sports\n",
       "17  alisson signed for liverpool fc from as roma t...   sports\n",
       "18  but the rankings during that run-in to new yor...   sports\n",
       "19  then came the oh-so-familiar djokovic-nadal no...   sports"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail() #주제가 sports인 지문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "5     None\n",
       "6     None\n",
       "7     None\n",
       "8     None\n",
       "9     None\n",
       "10    None\n",
       "11    None\n",
       "12    None\n",
       "13    None\n",
       "14    None\n",
       "15    None\n",
       "16    None\n",
       "17    None\n",
       "18    None\n",
       "19    None\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 전처리\n",
    "\n",
    "#1. 모든 단어들을 모아 중복을 제거한 후 단어 리스트 먼저 만들기\n",
    "results = set()\n",
    "df['paragraph'].str.lower().str.split().apply(results.update) # df['paragraph'] 문단을 소문자로 모두 전환 -> 띄어쓰기 기준으로 분류 -> result set에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(and',\n",
       " '1',\n",
       " '1.5',\n",
       " '1.5x',\n",
       " '10-9',\n",
       " '14',\n",
       " '2',\n",
       " '2014',\n",
       " '33/1',\n",
       " 'a',\n",
       " 'about',\n",
       " 'action',\n",
       " 'after',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ai',\n",
       " 'alisson',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'also',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'and',\n",
       " 'angelique',\n",
       " 'any',\n",
       " 'apps',\n",
       " 'are',\n",
       " 'area',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'australia',\n",
       " 'avoided',\n",
       " 'azarenka',\n",
       " 'back',\n",
       " 'battle',\n",
       " 'be',\n",
       " 'beef',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'began—too',\n",
       " 'being',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'biscuits',\n",
       " 'bit',\n",
       " 'bookmark',\n",
       " 'bookmarked',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'breakthroughs',\n",
       " 'briton',\n",
       " 'brunch',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'campaigns',\n",
       " 'can',\n",
       " 'card',\n",
       " 'caroline',\n",
       " 'case',\n",
       " 'certainly',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chasing',\n",
       " 'chicken',\n",
       " 'city',\n",
       " 'clay',\n",
       " 'clean',\n",
       " 'commented',\n",
       " 'competition',\n",
       " 'complain',\n",
       " 'confidence',\n",
       " 'contrast',\n",
       " 'could',\n",
       " 'course',\n",
       " 'courteous',\n",
       " 'courts',\n",
       " 'coworkers',\n",
       " 'crisp',\n",
       " 'croatia',\n",
       " 'crumbs',\n",
       " 'culminating',\n",
       " 'cup',\n",
       " 'currently',\n",
       " 'dare',\n",
       " 'days',\n",
       " 'decisive',\n",
       " 'defending',\n",
       " 'delivered',\n",
       " 'despite',\n",
       " 'did',\n",
       " 'didnt',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'dish',\n",
       " 'dishplace',\n",
       " 'dismal',\n",
       " 'displayed',\n",
       " 'djokovic',\n",
       " 'djokovic-nadal',\n",
       " 'does',\n",
       " 'doesnt',\n",
       " 'dog-friendly',\n",
       " 'dominance',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'down',\n",
       " 'downtown',\n",
       " 'dragged',\n",
       " 'drinks',\n",
       " 'during',\n",
       " 'eat',\n",
       " 'edged',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'eliminated',\n",
       " 'enduring',\n",
       " 'england',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enormously',\n",
       " 'entree',\n",
       " 'error',\n",
       " 'especially',\n",
       " 'even',\n",
       " 'event',\n",
       " 'excellent',\n",
       " 'exhilarating',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'extremely',\n",
       " 'face',\n",
       " 'failed',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fans',\n",
       " 'fc',\n",
       " 'fellow',\n",
       " 'final',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'first-round',\n",
       " 'five',\n",
       " 'flair',\n",
       " 'flavors',\n",
       " 'flight',\n",
       " 'food',\n",
       " 'for',\n",
       " 'form',\n",
       " 'formaggi',\n",
       " 'four',\n",
       " 'fourth',\n",
       " 'french',\n",
       " 'fries',\n",
       " 'from',\n",
       " 'full',\n",
       " 'games',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'generous',\n",
       " 'goalkeeper',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gravy',\n",
       " 'great',\n",
       " 'green',\n",
       " 'group',\n",
       " 'gruelling',\n",
       " 'had',\n",
       " 'halep',\n",
       " 'half',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hid',\n",
       " 'highly',\n",
       " 'hip',\n",
       " 'his',\n",
       " 'hodgson',\n",
       " 'hold',\n",
       " 'hope',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'however',\n",
       " 'hubby',\n",
       " 'huge',\n",
       " 'hungry',\n",
       " 'i',\n",
       " 'ice',\n",
       " 'iced',\n",
       " 'if',\n",
       " 'illness',\n",
       " 'important',\n",
       " 'improved',\n",
       " 'in',\n",
       " 'including',\n",
       " 'injury',\n",
       " 'is',\n",
       " 'it',\n",
       " 'italico',\n",
       " 'ivan',\n",
       " 'jelena',\n",
       " 'jonesing',\n",
       " 'jose',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kerber',\n",
       " 'kitchen',\n",
       " 'knock',\n",
       " 'know',\n",
       " 'konta',\n",
       " 'last',\n",
       " 'late',\n",
       " 'laughed',\n",
       " 'lead',\n",
       " 'league',\n",
       " 'led',\n",
       " 'left',\n",
       " 'leicester',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'liverpool',\n",
       " 'located',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'losing',\n",
       " 'losses',\n",
       " 'luka',\n",
       " 'lunchtime',\n",
       " 'madrid',\n",
       " 'major',\n",
       " 'make',\n",
       " 'making',\n",
       " 'manager',\n",
       " 'many',\n",
       " 'match',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'meal',\n",
       " 'miami',\n",
       " 'minutes',\n",
       " 'missed',\n",
       " 'modric',\n",
       " 'moment',\n",
       " 'months',\n",
       " 'montreal',\n",
       " 'montreal.',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'murray',\n",
       " 'my',\n",
       " 'nachos',\n",
       " 'nadal',\n",
       " 'names',\n",
       " 'nederer',\n",
       " 'net',\n",
       " 'never',\n",
       " 'new',\n",
       " 'nn7',\n",
       " 'no-quarter-given',\n",
       " 'no1',\n",
       " 'no2',\n",
       " 'no46',\n",
       " 'no83',\n",
       " 'north',\n",
       " 'not',\n",
       " 'odds',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh-so-familiar',\n",
       " 'omelettes',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'ordered',\n",
       " 'orders',\n",
       " 'ostapenko',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'palo',\n",
       " 'pancakes',\n",
       " 'parking',\n",
       " 'pasta',\n",
       " 'patience',\n",
       " 'peak',\n",
       " 'people',\n",
       " 'performances',\n",
       " 'perisic',\n",
       " 'perseyside',\n",
       " 'person',\n",
       " 'pessimistic',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plates',\n",
       " 'played',\n",
       " 'player',\n",
       " 'players',\n",
       " 'pleasantly',\n",
       " 'plus',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'possible',\n",
       " 'potential',\n",
       " 'premier',\n",
       " 'prepare',\n",
       " 'presentation',\n",
       " 'probably',\n",
       " 'promote',\n",
       " 'proved',\n",
       " 'pull',\n",
       " 'pulled',\n",
       " 'quality',\n",
       " 'quarter-final',\n",
       " 'quarter—except',\n",
       " 'quartet',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'rakitic',\n",
       " 'rallies',\n",
       " 'ran',\n",
       " 'ranked',\n",
       " 'ranking',\n",
       " 'rankings',\n",
       " 'ranks',\n",
       " 'reached',\n",
       " 'reaching',\n",
       " 'real',\n",
       " 'really',\n",
       " 'recommendations',\n",
       " 'recommended',\n",
       " 'reds',\n",
       " 'reigning',\n",
       " 'reservations',\n",
       " 'resignation',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'results',\n",
       " 'retrieving',\n",
       " 'return',\n",
       " 'returning',\n",
       " 'reverse',\n",
       " 'rich',\n",
       " 'roma',\n",
       " 'round',\n",
       " 'row',\n",
       " 'roy',\n",
       " 'run-in',\n",
       " 'russia',\n",
       " 'said',\n",
       " 'salty',\n",
       " 'san',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'say',\n",
       " 'scored',\n",
       " 'seal',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seeded',\n",
       " 'seeding',\n",
       " 'seem',\n",
       " 'semi-final',\n",
       " 'semis',\n",
       " 'separate',\n",
       " 'serena',\n",
       " 'served',\n",
       " 'server',\n",
       " 'serves',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'set',\n",
       " 'she',\n",
       " 'sheets',\n",
       " 'shots',\n",
       " 'showed',\n",
       " 'side',\n",
       " 'signed',\n",
       " 'signs',\n",
       " 'simona',\n",
       " 'since',\n",
       " 'sisters',\n",
       " 'sliders',\n",
       " 'slides',\n",
       " 'sloane',\n",
       " 'slow',\n",
       " 'slower',\n",
       " 'slump',\n",
       " 'small',\n",
       " 'smash',\n",
       " 'so',\n",
       " 'soft',\n",
       " 'some',\n",
       " 'soon',\n",
       " 'special)',\n",
       " 'spot',\n",
       " 'stage',\n",
       " 'star',\n",
       " 'started',\n",
       " 'starter',\n",
       " 'stephens',\n",
       " 'straight',\n",
       " 'struggled',\n",
       " 'stunning',\n",
       " 'style',\n",
       " 'sub-par',\n",
       " 'succeed',\n",
       " 'summer',\n",
       " 'sunnyvale',\n",
       " 'surged',\n",
       " 'surprised',\n",
       " 'sweet',\n",
       " 'swing',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'term',\n",
       " 'tests',\n",
       " 'texture',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'things',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'tie-break',\n",
       " 'time',\n",
       " 'title',\n",
       " 'to',\n",
       " 'toast',\n",
       " 'too',\n",
       " 'took',\n",
       " 'top',\n",
       " 'tough',\n",
       " 'tournament',\n",
       " 'tournaments',\n",
       " 'travel',\n",
       " 'trophy',\n",
       " 'truly',\n",
       " 'turnaround',\n",
       " 'two',\n",
       " 'two-sets',\n",
       " 'undercurrents',\n",
       " 'unless',\n",
       " 'up',\n",
       " 'us',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'victoria',\n",
       " 'victory',\n",
       " 'visit',\n",
       " 'waffle',\n",
       " 'wait',\n",
       " 'waiter',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'was',\n",
       " 'wasnt',\n",
       " 'way',\n",
       " 'we',\n",
       " 'week',\n",
       " 'weekday',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'who',\n",
       " 'why',\n",
       " 'wild',\n",
       " 'will',\n",
       " 'williams',\n",
       " 'wimbledon',\n",
       " 'win',\n",
       " 'winning',\n",
       " 'wins',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonderful',\n",
       " 'world',\n",
       " 'would',\n",
       " 'wozniacki',\n",
       " 'year',\n",
       " 'years',\n",
       " 'year—and',\n",
       " 'york',\n",
       " 'you',\n",
       " '–'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 딕셔너리 만들기\n",
    "idx2word = dict(enumerate(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '–',\n",
       " 1: 'soft',\n",
       " 2: 'fans',\n",
       " 3: 'jelena',\n",
       " 4: 'outside',\n",
       " 5: 'final',\n",
       " 6: 'third',\n",
       " 7: 'first-round',\n",
       " 8: 'ranked',\n",
       " 9: 'exhilarating',\n",
       " 10: 'brunch',\n",
       " 11: 'sauce',\n",
       " 12: 'potential',\n",
       " 13: 'italico',\n",
       " 14: 'sweet',\n",
       " 15: 'lunchtime',\n",
       " 16: 'were',\n",
       " 17: 'as',\n",
       " 18: 'took',\n",
       " 19: 'injury',\n",
       " 20: 'quarter-final',\n",
       " 21: 'oh-so-familiar',\n",
       " 22: 'happy',\n",
       " 23: 'peak',\n",
       " 24: 'busy',\n",
       " 25: 'open',\n",
       " 26: 'dare',\n",
       " 27: 'fourth',\n",
       " 28: 'error',\n",
       " 29: 'edged',\n",
       " 30: 'generous',\n",
       " 31: 'angelique',\n",
       " 32: 'left',\n",
       " 33: 'montreal.',\n",
       " 34: 'quarter—except',\n",
       " 35: 'then',\n",
       " 36: 'big',\n",
       " 37: 'never',\n",
       " 38: 'people',\n",
       " 39: 'fairly',\n",
       " 40: 'russia',\n",
       " 41: 'patience',\n",
       " 42: 'fellow',\n",
       " 43: 'tournaments',\n",
       " 44: 'enjoy',\n",
       " 45: 'djokovic',\n",
       " 46: 'started',\n",
       " 47: 'year—and',\n",
       " 48: 'our',\n",
       " 49: 'could',\n",
       " 50: 'two-sets',\n",
       " 51: 'at',\n",
       " 52: 'avoided',\n",
       " 53: 'seeding',\n",
       " 54: 'much',\n",
       " 55: 'victory',\n",
       " 56: 'portions',\n",
       " 57: 'fc',\n",
       " 58: 'tasted',\n",
       " 59: 'been',\n",
       " 60: 'promote',\n",
       " 61: 'flair',\n",
       " 62: 'seating',\n",
       " 63: 'dishplace',\n",
       " 64: 'no-quarter-given',\n",
       " 65: 'between',\n",
       " 66: 'hope',\n",
       " 67: 'surged',\n",
       " 68: 'really',\n",
       " 69: 'haven',\n",
       " 70: 'winning',\n",
       " 71: 'seated',\n",
       " 72: 'full',\n",
       " 73: 'taste',\n",
       " 74: 'and',\n",
       " 75: 'murray',\n",
       " 76: 'term',\n",
       " 77: 'cup',\n",
       " 78: 'possible',\n",
       " 79: 'kitchen',\n",
       " 80: 'soon',\n",
       " 81: 'just',\n",
       " 82: 'prepare',\n",
       " 83: 'breakthroughs',\n",
       " 84: 'wonderful',\n",
       " 85: 'san',\n",
       " 86: 'york',\n",
       " 87: 'expensive',\n",
       " 88: 'got',\n",
       " 89: 'of',\n",
       " 90: 'wins',\n",
       " 91: 'madrid',\n",
       " 92: 'days',\n",
       " 93: 'reverse',\n",
       " 94: 'we',\n",
       " 95: 'enduring',\n",
       " 96: 'face',\n",
       " 97: 'parking',\n",
       " 98: 'thought',\n",
       " 99: 'since',\n",
       " 100: 'excellent',\n",
       " 101: 'us',\n",
       " 102: 'quick',\n",
       " 103: 'almost',\n",
       " 104: 'knock',\n",
       " 105: 'various',\n",
       " 106: 'huge',\n",
       " 107: 'shots',\n",
       " 108: 'serves',\n",
       " 109: 'turnaround',\n",
       " 110: 'montreal',\n",
       " 111: 'hodgson',\n",
       " 112: 'plus',\n",
       " 113: 'courts',\n",
       " 114: 'important',\n",
       " 115: 'pasta',\n",
       " 116: 'alisson',\n",
       " 117: 'going',\n",
       " 118: 'seal',\n",
       " 119: 'serena',\n",
       " 120: 'displayed',\n",
       " 121: 'fair',\n",
       " 122: 'stunning',\n",
       " 123: 'in',\n",
       " 124: 'minutes',\n",
       " 125: 'ago',\n",
       " 126: 'hid',\n",
       " 127: 'slump',\n",
       " 128: 'pancakes',\n",
       " 129: 'eliminated',\n",
       " 130: 'from',\n",
       " 131: 'out',\n",
       " 132: 'recommended',\n",
       " 133: 'doesnt',\n",
       " 134: 'enjoyed',\n",
       " 135: 'goalkeeper',\n",
       " 136: 'semi-final',\n",
       " 137: 'top',\n",
       " 138: 'way',\n",
       " 139: 'style',\n",
       " 140: '2014',\n",
       " 141: 'late',\n",
       " 142: 'losing',\n",
       " 143: 'campaigns',\n",
       " 144: 'signs',\n",
       " 145: 'some',\n",
       " 146: 'outfit',\n",
       " 147: 'smash',\n",
       " 148: 'very',\n",
       " 149: 'here',\n",
       " 150: 'dog-friendly',\n",
       " 151: 'have',\n",
       " 152: 'roy',\n",
       " 153: 'chicken',\n",
       " 154: 'player',\n",
       " 155: 'better',\n",
       " 156: 'contrast',\n",
       " 157: 'jose',\n",
       " 158: 'roma',\n",
       " 159: 'struggled',\n",
       " 160: 'would',\n",
       " 161: 'pessimistic',\n",
       " 162: 'retrieving',\n",
       " 163: 'many',\n",
       " 164: 'against',\n",
       " 165: 'meal',\n",
       " 166: 'formaggi',\n",
       " 167: '1.5',\n",
       " 168: 'seeded',\n",
       " 169: 'flight',\n",
       " 170: 'making',\n",
       " 171: 'about',\n",
       " 172: 'wanted',\n",
       " 173: 'returning',\n",
       " 174: 'they',\n",
       " 175: 'taken',\n",
       " 176: 'spot',\n",
       " 177: 'reservations',\n",
       " 178: 'helped',\n",
       " 179: '1',\n",
       " 180: 'hip',\n",
       " 181: 'make',\n",
       " 182: 'small',\n",
       " 183: 'ostapenko',\n",
       " 184: 'me',\n",
       " 185: 'came',\n",
       " 186: 'performances',\n",
       " 187: 'chasing',\n",
       " 188: 'downtown',\n",
       " 189: 'slow',\n",
       " 190: 'back',\n",
       " 191: 'pull',\n",
       " 192: 'rallies',\n",
       " 193: 'title',\n",
       " 194: 'so',\n",
       " 195: 'week',\n",
       " 196: 'slower',\n",
       " 197: 'liverpool',\n",
       " 198: 'other',\n",
       " 199: 'tea',\n",
       " 200: 'difficult',\n",
       " 201: 'texture',\n",
       " 202: 'caroline',\n",
       " 203: 'toast',\n",
       " 204: 'food',\n",
       " 205: 'dismal',\n",
       " 206: 'only',\n",
       " 207: 'this',\n",
       " 208: 'chances',\n",
       " 209: 'months',\n",
       " 210: 'if',\n",
       " 211: 'culminating',\n",
       " 212: 'even',\n",
       " 213: 'card',\n",
       " 214: 'entree',\n",
       " 215: 'sisters',\n",
       " 216: 'case',\n",
       " 217: 'quartet',\n",
       " 218: 'set',\n",
       " 219: 'years',\n",
       " 220: 'for',\n",
       " 221: 'beef',\n",
       " 222: 'premier',\n",
       " 223: 'confidence',\n",
       " 224: 'salty',\n",
       " 225: 'including',\n",
       " 226: 'failed',\n",
       " 227: 'iced',\n",
       " 228: 'separate',\n",
       " 229: 'croatia',\n",
       " 230: 'last',\n",
       " 231: 'which',\n",
       " 232: 'to',\n",
       " 233: 'dont',\n",
       " 234: 'but',\n",
       " 235: 'simona',\n",
       " 236: 'straight',\n",
       " 237: 'know',\n",
       " 238: 'help',\n",
       " 239: 'garlic',\n",
       " 240: 'ran',\n",
       " 241: 'three',\n",
       " 242: 'pulled',\n",
       " 243: 'good',\n",
       " 244: 'briton',\n",
       " 245: 'any',\n",
       " 246: 'maybe',\n",
       " 247: 'starter',\n",
       " 248: 'djokovic-nadal',\n",
       " 249: 'perseyside',\n",
       " 250: 'clay',\n",
       " 251: 'slides',\n",
       " 252: 'where',\n",
       " 253: 'star',\n",
       " 254: 'she',\n",
       " 255: 'apps',\n",
       " 256: '2',\n",
       " 257: 'north',\n",
       " 258: 'side',\n",
       " 259: 'azarenka',\n",
       " 260: 'major',\n",
       " 261: 'nederer',\n",
       " 262: 'defending',\n",
       " 263: 'sheets',\n",
       " 264: 'competition',\n",
       " 265: 'fries',\n",
       " 266: 'enormously',\n",
       " 267: 'extremely',\n",
       " 268: 'palo',\n",
       " 269: 'before',\n",
       " 270: 'event',\n",
       " 271: 'waiter',\n",
       " 272: 'their',\n",
       " 273: 'league',\n",
       " 274: 'american',\n",
       " 275: 'reaching',\n",
       " 276: 'is',\n",
       " 277: 'being',\n",
       " 278: 'having',\n",
       " 279: 'probably',\n",
       " 280: 'didnt',\n",
       " 281: 'reigning',\n",
       " 282: 'illness',\n",
       " 283: 'can',\n",
       " 284: 'ai',\n",
       " 285: 'bottom',\n",
       " 286: 'jonesing',\n",
       " 287: '10-9',\n",
       " 288: 'world',\n",
       " 289: 'up',\n",
       " 290: 'be',\n",
       " 291: 'waffle',\n",
       " 292: 'team',\n",
       " 293: 'that',\n",
       " 294: 'dinner',\n",
       " 295: 'the',\n",
       " 296: 'laughed',\n",
       " 297: 'showed',\n",
       " 298: 'quickly',\n",
       " 299: 'odds',\n",
       " 300: 'he',\n",
       " 301: 'gave',\n",
       " 302: 'wimbledon',\n",
       " 303: 'off',\n",
       " 304: 'perisic',\n",
       " 305: 'four',\n",
       " 306: 'time',\n",
       " 307: 'hour',\n",
       " 308: 'long',\n",
       " 309: 'does',\n",
       " 310: 'located',\n",
       " 311: 'biscuits',\n",
       " 312: 'finished',\n",
       " 313: 'leicester',\n",
       " 314: 'served',\n",
       " 315: 'keep',\n",
       " 316: 'seem',\n",
       " 317: 'there',\n",
       " 318: 'nachos',\n",
       " 319: 'wild',\n",
       " 320: 'server',\n",
       " 321: 'places',\n",
       " 322: 'no2',\n",
       " 323: 'via',\n",
       " 324: 'more',\n",
       " 325: '14',\n",
       " 326: 'find',\n",
       " 327: 'egg',\n",
       " 328: 'business',\n",
       " 329: 'within',\n",
       " 330: 'his',\n",
       " 331: 'brazilian',\n",
       " 332: 'rankings',\n",
       " 333: 'omelettes',\n",
       " 334: 'improved',\n",
       " 335: 'amazing',\n",
       " 336: 'you',\n",
       " 337: 'expected',\n",
       " 338: 'win',\n",
       " 339: 'certainly',\n",
       " 340: 'scored',\n",
       " 341: 'gravy',\n",
       " 342: 'over',\n",
       " 343: 'serving',\n",
       " 344: 'hungry',\n",
       " 345: 'a',\n",
       " 346: 'opened',\n",
       " 347: 'form',\n",
       " 348: 'who',\n",
       " 349: 'wozniacki',\n",
       " 350: 'why',\n",
       " 351: 'her',\n",
       " 352: 'proved',\n",
       " 353: 'however',\n",
       " 354: 'ice',\n",
       " 355: 'was',\n",
       " 356: 'row',\n",
       " 357: 'semis',\n",
       " 358: 'tough',\n",
       " 359: 'england',\n",
       " 360: 'decisive',\n",
       " 361: 'plates',\n",
       " 362: 'presentation',\n",
       " 363: 'kerber',\n",
       " 364: 'also',\n",
       " 365: 'ranks',\n",
       " 366: 'say',\n",
       " 367: 'no46',\n",
       " 368: 'names',\n",
       " 369: 'signed',\n",
       " 370: 'after',\n",
       " 371: 'person',\n",
       " 372: 'dish',\n",
       " 373: 'commented',\n",
       " 374: 'had',\n",
       " 375: 'green',\n",
       " 376: 'place',\n",
       " 377: 'losses',\n",
       " 378: 'miami',\n",
       " 379: 'area',\n",
       " 380: 'visit',\n",
       " 381: 'portion',\n",
       " 382: 'konta',\n",
       " 383: 'those',\n",
       " 384: 'different',\n",
       " 385: 'pleasantly',\n",
       " 386: 'lead',\n",
       " 387: 'table',\n",
       " 388: 'did',\n",
       " 389: 'nadal',\n",
       " 390: 'led',\n",
       " 391: 'hard',\n",
       " 392: 'one',\n",
       " 393: 'sat',\n",
       " 394: 'all',\n",
       " 395: 'unless',\n",
       " 396: 'around',\n",
       " 397: 'travel',\n",
       " 398: 'went',\n",
       " 399: '33/1',\n",
       " 400: 'too',\n",
       " 401: 'crumbs',\n",
       " 402: 'when',\n",
       " 403: 'by',\n",
       " 404: 'course',\n",
       " 405: 'city',\n",
       " 406: 'on',\n",
       " 407: 'champions',\n",
       " 408: 'gruelling',\n",
       " 409: 'without',\n",
       " 410: 'bit',\n",
       " 411: 'finally',\n",
       " 412: 'wait',\n",
       " 413: 'round',\n",
       " 414: 'year',\n",
       " 415: 'complain',\n",
       " 416: '1.5x',\n",
       " 417: 'results',\n",
       " 418: 'crisp',\n",
       " 419: 'luka',\n",
       " 420: 'action',\n",
       " 421: 'look',\n",
       " 422: 'it',\n",
       " 423: 'nn7',\n",
       " 424: 'half',\n",
       " 425: 'reds',\n",
       " 426: 'return',\n",
       " 427: 'hold',\n",
       " 428: 'tournament',\n",
       " 429: 'rakitic',\n",
       " 430: 'tests',\n",
       " 431: 'ordered',\n",
       " 432: 'likely',\n",
       " 433: 'special)',\n",
       " 434: 'summer',\n",
       " 435: 'are',\n",
       " 436: 'orders',\n",
       " 437: 'clean',\n",
       " 438: 'coworkers',\n",
       " 439: 'victoria',\n",
       " 440: 'looked',\n",
       " 441: 'down',\n",
       " 442: 'sub-par',\n",
       " 443: 'succeed',\n",
       " 444: 'double',\n",
       " 445: 'stephens',\n",
       " 446: 'eggs',\n",
       " 447: 'two',\n",
       " 448: 'hubby',\n",
       " 449: 'manager',\n",
       " 450: 'will',\n",
       " 451: 'highly',\n",
       " 452: 'net',\n",
       " 453: 'service',\n",
       " 454: 'eat',\n",
       " 455: 'alto',\n",
       " 456: 'swing',\n",
       " 457: 'not',\n",
       " 458: 'bread',\n",
       " 459: 'despite',\n",
       " 460: 'restaurants',\n",
       " 461: 'has',\n",
       " 462: 'bookmarked',\n",
       " 463: 'began—too',\n",
       " 464: 'especially',\n",
       " 465: 'undercurrents',\n",
       " 466: 'missed',\n",
       " 467: 'halep',\n",
       " 468: 'real',\n",
       " 469: 'dragged',\n",
       " 470: 'i',\n",
       " 471: 'during',\n",
       " 472: 'trophy',\n",
       " 473: 'restaurant',\n",
       " 474: 'what',\n",
       " 475: 'truly',\n",
       " 476: 'played',\n",
       " 477: 'flavors',\n",
       " 478: 'ranking',\n",
       " 479: 'bookmark',\n",
       " 480: 'dominance',\n",
       " 481: 'resignation',\n",
       " 482: 'rich',\n",
       " 483: 'five',\n",
       " 484: 'great',\n",
       " 485: 'first',\n",
       " 486: 'weekday',\n",
       " 487: 'delivered',\n",
       " 488: 'most',\n",
       " 489: 'although',\n",
       " 490: 'battle',\n",
       " 491: 'saturday',\n",
       " 492: 'both',\n",
       " 493: 'courteous',\n",
       " 494: 'said',\n",
       " 495: 'things',\n",
       " 496: 'recommendations',\n",
       " 497: 'champion',\n",
       " 498: 'with',\n",
       " 499: 'no1',\n",
       " 500: 'ivan',\n",
       " 501: 'sloane',\n",
       " 502: 'surprised',\n",
       " 503: '(and',\n",
       " 504: 'wasnt',\n",
       " 505: 'my',\n",
       " 506: 'sunnyvale',\n",
       " 507: 'moment',\n",
       " 508: 'group',\n",
       " 509: 'chance',\n",
       " 510: 'currently',\n",
       " 511: 'sliders',\n",
       " 512: 'quality',\n",
       " 513: 'drinks',\n",
       " 514: 'themselves',\n",
       " 515: 'once',\n",
       " 516: 'began',\n",
       " 517: 'run-in',\n",
       " 518: 'burgers',\n",
       " 519: 'no83',\n",
       " 520: 'match',\n",
       " 521: 'stage',\n",
       " 522: 'new',\n",
       " 523: 'modric',\n",
       " 524: 'like',\n",
       " 525: 'players',\n",
       " 526: 'burger',\n",
       " 527: 'australia',\n",
       " 528: 'hours',\n",
       " 529: 'than',\n",
       " 530: 'tie-break',\n",
       " 531: 'games',\n",
       " 532: 'reached',\n",
       " 533: 'french',\n",
       " 534: 'want',\n",
       " 535: 'williams'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {v:k for k, v in idx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'–': 0,\n",
       " 'soft': 1,\n",
       " 'fans': 2,\n",
       " 'jelena': 3,\n",
       " 'outside': 4,\n",
       " 'final': 5,\n",
       " 'third': 6,\n",
       " 'first-round': 7,\n",
       " 'ranked': 8,\n",
       " 'exhilarating': 9,\n",
       " 'brunch': 10,\n",
       " 'sauce': 11,\n",
       " 'potential': 12,\n",
       " 'italico': 13,\n",
       " 'sweet': 14,\n",
       " 'lunchtime': 15,\n",
       " 'were': 16,\n",
       " 'as': 17,\n",
       " 'took': 18,\n",
       " 'injury': 19,\n",
       " 'quarter-final': 20,\n",
       " 'oh-so-familiar': 21,\n",
       " 'happy': 22,\n",
       " 'peak': 23,\n",
       " 'busy': 24,\n",
       " 'open': 25,\n",
       " 'dare': 26,\n",
       " 'fourth': 27,\n",
       " 'error': 28,\n",
       " 'edged': 29,\n",
       " 'generous': 30,\n",
       " 'angelique': 31,\n",
       " 'left': 32,\n",
       " 'montreal.': 33,\n",
       " 'quarter—except': 34,\n",
       " 'then': 35,\n",
       " 'big': 36,\n",
       " 'never': 37,\n",
       " 'people': 38,\n",
       " 'fairly': 39,\n",
       " 'russia': 40,\n",
       " 'patience': 41,\n",
       " 'fellow': 42,\n",
       " 'tournaments': 43,\n",
       " 'enjoy': 44,\n",
       " 'djokovic': 45,\n",
       " 'started': 46,\n",
       " 'year—and': 47,\n",
       " 'our': 48,\n",
       " 'could': 49,\n",
       " 'two-sets': 50,\n",
       " 'at': 51,\n",
       " 'avoided': 52,\n",
       " 'seeding': 53,\n",
       " 'much': 54,\n",
       " 'victory': 55,\n",
       " 'portions': 56,\n",
       " 'fc': 57,\n",
       " 'tasted': 58,\n",
       " 'been': 59,\n",
       " 'promote': 60,\n",
       " 'flair': 61,\n",
       " 'seating': 62,\n",
       " 'dishplace': 63,\n",
       " 'no-quarter-given': 64,\n",
       " 'between': 65,\n",
       " 'hope': 66,\n",
       " 'surged': 67,\n",
       " 'really': 68,\n",
       " 'haven': 69,\n",
       " 'winning': 70,\n",
       " 'seated': 71,\n",
       " 'full': 72,\n",
       " 'taste': 73,\n",
       " 'and': 74,\n",
       " 'murray': 75,\n",
       " 'term': 76,\n",
       " 'cup': 77,\n",
       " 'possible': 78,\n",
       " 'kitchen': 79,\n",
       " 'soon': 80,\n",
       " 'just': 81,\n",
       " 'prepare': 82,\n",
       " 'breakthroughs': 83,\n",
       " 'wonderful': 84,\n",
       " 'san': 85,\n",
       " 'york': 86,\n",
       " 'expensive': 87,\n",
       " 'got': 88,\n",
       " 'of': 89,\n",
       " 'wins': 90,\n",
       " 'madrid': 91,\n",
       " 'days': 92,\n",
       " 'reverse': 93,\n",
       " 'we': 94,\n",
       " 'enduring': 95,\n",
       " 'face': 96,\n",
       " 'parking': 97,\n",
       " 'thought': 98,\n",
       " 'since': 99,\n",
       " 'excellent': 100,\n",
       " 'us': 101,\n",
       " 'quick': 102,\n",
       " 'almost': 103,\n",
       " 'knock': 104,\n",
       " 'various': 105,\n",
       " 'huge': 106,\n",
       " 'shots': 107,\n",
       " 'serves': 108,\n",
       " 'turnaround': 109,\n",
       " 'montreal': 110,\n",
       " 'hodgson': 111,\n",
       " 'plus': 112,\n",
       " 'courts': 113,\n",
       " 'important': 114,\n",
       " 'pasta': 115,\n",
       " 'alisson': 116,\n",
       " 'going': 117,\n",
       " 'seal': 118,\n",
       " 'serena': 119,\n",
       " 'displayed': 120,\n",
       " 'fair': 121,\n",
       " 'stunning': 122,\n",
       " 'in': 123,\n",
       " 'minutes': 124,\n",
       " 'ago': 125,\n",
       " 'hid': 126,\n",
       " 'slump': 127,\n",
       " 'pancakes': 128,\n",
       " 'eliminated': 129,\n",
       " 'from': 130,\n",
       " 'out': 131,\n",
       " 'recommended': 132,\n",
       " 'doesnt': 133,\n",
       " 'enjoyed': 134,\n",
       " 'goalkeeper': 135,\n",
       " 'semi-final': 136,\n",
       " 'top': 137,\n",
       " 'way': 138,\n",
       " 'style': 139,\n",
       " '2014': 140,\n",
       " 'late': 141,\n",
       " 'losing': 142,\n",
       " 'campaigns': 143,\n",
       " 'signs': 144,\n",
       " 'some': 145,\n",
       " 'outfit': 146,\n",
       " 'smash': 147,\n",
       " 'very': 148,\n",
       " 'here': 149,\n",
       " 'dog-friendly': 150,\n",
       " 'have': 151,\n",
       " 'roy': 152,\n",
       " 'chicken': 153,\n",
       " 'player': 154,\n",
       " 'better': 155,\n",
       " 'contrast': 156,\n",
       " 'jose': 157,\n",
       " 'roma': 158,\n",
       " 'struggled': 159,\n",
       " 'would': 160,\n",
       " 'pessimistic': 161,\n",
       " 'retrieving': 162,\n",
       " 'many': 163,\n",
       " 'against': 164,\n",
       " 'meal': 165,\n",
       " 'formaggi': 166,\n",
       " '1.5': 167,\n",
       " 'seeded': 168,\n",
       " 'flight': 169,\n",
       " 'making': 170,\n",
       " 'about': 171,\n",
       " 'wanted': 172,\n",
       " 'returning': 173,\n",
       " 'they': 174,\n",
       " 'taken': 175,\n",
       " 'spot': 176,\n",
       " 'reservations': 177,\n",
       " 'helped': 178,\n",
       " '1': 179,\n",
       " 'hip': 180,\n",
       " 'make': 181,\n",
       " 'small': 182,\n",
       " 'ostapenko': 183,\n",
       " 'me': 184,\n",
       " 'came': 185,\n",
       " 'performances': 186,\n",
       " 'chasing': 187,\n",
       " 'downtown': 188,\n",
       " 'slow': 189,\n",
       " 'back': 190,\n",
       " 'pull': 191,\n",
       " 'rallies': 192,\n",
       " 'title': 193,\n",
       " 'so': 194,\n",
       " 'week': 195,\n",
       " 'slower': 196,\n",
       " 'liverpool': 197,\n",
       " 'other': 198,\n",
       " 'tea': 199,\n",
       " 'difficult': 200,\n",
       " 'texture': 201,\n",
       " 'caroline': 202,\n",
       " 'toast': 203,\n",
       " 'food': 204,\n",
       " 'dismal': 205,\n",
       " 'only': 206,\n",
       " 'this': 207,\n",
       " 'chances': 208,\n",
       " 'months': 209,\n",
       " 'if': 210,\n",
       " 'culminating': 211,\n",
       " 'even': 212,\n",
       " 'card': 213,\n",
       " 'entree': 214,\n",
       " 'sisters': 215,\n",
       " 'case': 216,\n",
       " 'quartet': 217,\n",
       " 'set': 218,\n",
       " 'years': 219,\n",
       " 'for': 220,\n",
       " 'beef': 221,\n",
       " 'premier': 222,\n",
       " 'confidence': 223,\n",
       " 'salty': 224,\n",
       " 'including': 225,\n",
       " 'failed': 226,\n",
       " 'iced': 227,\n",
       " 'separate': 228,\n",
       " 'croatia': 229,\n",
       " 'last': 230,\n",
       " 'which': 231,\n",
       " 'to': 232,\n",
       " 'dont': 233,\n",
       " 'but': 234,\n",
       " 'simona': 235,\n",
       " 'straight': 236,\n",
       " 'know': 237,\n",
       " 'help': 238,\n",
       " 'garlic': 239,\n",
       " 'ran': 240,\n",
       " 'three': 241,\n",
       " 'pulled': 242,\n",
       " 'good': 243,\n",
       " 'briton': 244,\n",
       " 'any': 245,\n",
       " 'maybe': 246,\n",
       " 'starter': 247,\n",
       " 'djokovic-nadal': 248,\n",
       " 'perseyside': 249,\n",
       " 'clay': 250,\n",
       " 'slides': 251,\n",
       " 'where': 252,\n",
       " 'star': 253,\n",
       " 'she': 254,\n",
       " 'apps': 255,\n",
       " '2': 256,\n",
       " 'north': 257,\n",
       " 'side': 258,\n",
       " 'azarenka': 259,\n",
       " 'major': 260,\n",
       " 'nederer': 261,\n",
       " 'defending': 262,\n",
       " 'sheets': 263,\n",
       " 'competition': 264,\n",
       " 'fries': 265,\n",
       " 'enormously': 266,\n",
       " 'extremely': 267,\n",
       " 'palo': 268,\n",
       " 'before': 269,\n",
       " 'event': 270,\n",
       " 'waiter': 271,\n",
       " 'their': 272,\n",
       " 'league': 273,\n",
       " 'american': 274,\n",
       " 'reaching': 275,\n",
       " 'is': 276,\n",
       " 'being': 277,\n",
       " 'having': 278,\n",
       " 'probably': 279,\n",
       " 'didnt': 280,\n",
       " 'reigning': 281,\n",
       " 'illness': 282,\n",
       " 'can': 283,\n",
       " 'ai': 284,\n",
       " 'bottom': 285,\n",
       " 'jonesing': 286,\n",
       " '10-9': 287,\n",
       " 'world': 288,\n",
       " 'up': 289,\n",
       " 'be': 290,\n",
       " 'waffle': 291,\n",
       " 'team': 292,\n",
       " 'that': 293,\n",
       " 'dinner': 294,\n",
       " 'the': 295,\n",
       " 'laughed': 296,\n",
       " 'showed': 297,\n",
       " 'quickly': 298,\n",
       " 'odds': 299,\n",
       " 'he': 300,\n",
       " 'gave': 301,\n",
       " 'wimbledon': 302,\n",
       " 'off': 303,\n",
       " 'perisic': 304,\n",
       " 'four': 305,\n",
       " 'time': 306,\n",
       " 'hour': 307,\n",
       " 'long': 308,\n",
       " 'does': 309,\n",
       " 'located': 310,\n",
       " 'biscuits': 311,\n",
       " 'finished': 312,\n",
       " 'leicester': 313,\n",
       " 'served': 314,\n",
       " 'keep': 315,\n",
       " 'seem': 316,\n",
       " 'there': 317,\n",
       " 'nachos': 318,\n",
       " 'wild': 319,\n",
       " 'server': 320,\n",
       " 'places': 321,\n",
       " 'no2': 322,\n",
       " 'via': 323,\n",
       " 'more': 324,\n",
       " '14': 325,\n",
       " 'find': 326,\n",
       " 'egg': 327,\n",
       " 'business': 328,\n",
       " 'within': 329,\n",
       " 'his': 330,\n",
       " 'brazilian': 331,\n",
       " 'rankings': 332,\n",
       " 'omelettes': 333,\n",
       " 'improved': 334,\n",
       " 'amazing': 335,\n",
       " 'you': 336,\n",
       " 'expected': 337,\n",
       " 'win': 338,\n",
       " 'certainly': 339,\n",
       " 'scored': 340,\n",
       " 'gravy': 341,\n",
       " 'over': 342,\n",
       " 'serving': 343,\n",
       " 'hungry': 344,\n",
       " 'a': 345,\n",
       " 'opened': 346,\n",
       " 'form': 347,\n",
       " 'who': 348,\n",
       " 'wozniacki': 349,\n",
       " 'why': 350,\n",
       " 'her': 351,\n",
       " 'proved': 352,\n",
       " 'however': 353,\n",
       " 'ice': 354,\n",
       " 'was': 355,\n",
       " 'row': 356,\n",
       " 'semis': 357,\n",
       " 'tough': 358,\n",
       " 'england': 359,\n",
       " 'decisive': 360,\n",
       " 'plates': 361,\n",
       " 'presentation': 362,\n",
       " 'kerber': 363,\n",
       " 'also': 364,\n",
       " 'ranks': 365,\n",
       " 'say': 366,\n",
       " 'no46': 367,\n",
       " 'names': 368,\n",
       " 'signed': 369,\n",
       " 'after': 370,\n",
       " 'person': 371,\n",
       " 'dish': 372,\n",
       " 'commented': 373,\n",
       " 'had': 374,\n",
       " 'green': 375,\n",
       " 'place': 376,\n",
       " 'losses': 377,\n",
       " 'miami': 378,\n",
       " 'area': 379,\n",
       " 'visit': 380,\n",
       " 'portion': 381,\n",
       " 'konta': 382,\n",
       " 'those': 383,\n",
       " 'different': 384,\n",
       " 'pleasantly': 385,\n",
       " 'lead': 386,\n",
       " 'table': 387,\n",
       " 'did': 388,\n",
       " 'nadal': 389,\n",
       " 'led': 390,\n",
       " 'hard': 391,\n",
       " 'one': 392,\n",
       " 'sat': 393,\n",
       " 'all': 394,\n",
       " 'unless': 395,\n",
       " 'around': 396,\n",
       " 'travel': 397,\n",
       " 'went': 398,\n",
       " '33/1': 399,\n",
       " 'too': 400,\n",
       " 'crumbs': 401,\n",
       " 'when': 402,\n",
       " 'by': 403,\n",
       " 'course': 404,\n",
       " 'city': 405,\n",
       " 'on': 406,\n",
       " 'champions': 407,\n",
       " 'gruelling': 408,\n",
       " 'without': 409,\n",
       " 'bit': 410,\n",
       " 'finally': 411,\n",
       " 'wait': 412,\n",
       " 'round': 413,\n",
       " 'year': 414,\n",
       " 'complain': 415,\n",
       " '1.5x': 416,\n",
       " 'results': 417,\n",
       " 'crisp': 418,\n",
       " 'luka': 419,\n",
       " 'action': 420,\n",
       " 'look': 421,\n",
       " 'it': 422,\n",
       " 'nn7': 423,\n",
       " 'half': 424,\n",
       " 'reds': 425,\n",
       " 'return': 426,\n",
       " 'hold': 427,\n",
       " 'tournament': 428,\n",
       " 'rakitic': 429,\n",
       " 'tests': 430,\n",
       " 'ordered': 431,\n",
       " 'likely': 432,\n",
       " 'special)': 433,\n",
       " 'summer': 434,\n",
       " 'are': 435,\n",
       " 'orders': 436,\n",
       " 'clean': 437,\n",
       " 'coworkers': 438,\n",
       " 'victoria': 439,\n",
       " 'looked': 440,\n",
       " 'down': 441,\n",
       " 'sub-par': 442,\n",
       " 'succeed': 443,\n",
       " 'double': 444,\n",
       " 'stephens': 445,\n",
       " 'eggs': 446,\n",
       " 'two': 447,\n",
       " 'hubby': 448,\n",
       " 'manager': 449,\n",
       " 'will': 450,\n",
       " 'highly': 451,\n",
       " 'net': 452,\n",
       " 'service': 453,\n",
       " 'eat': 454,\n",
       " 'alto': 455,\n",
       " 'swing': 456,\n",
       " 'not': 457,\n",
       " 'bread': 458,\n",
       " 'despite': 459,\n",
       " 'restaurants': 460,\n",
       " 'has': 461,\n",
       " 'bookmarked': 462,\n",
       " 'began—too': 463,\n",
       " 'especially': 464,\n",
       " 'undercurrents': 465,\n",
       " 'missed': 466,\n",
       " 'halep': 467,\n",
       " 'real': 468,\n",
       " 'dragged': 469,\n",
       " 'i': 470,\n",
       " 'during': 471,\n",
       " 'trophy': 472,\n",
       " 'restaurant': 473,\n",
       " 'what': 474,\n",
       " 'truly': 475,\n",
       " 'played': 476,\n",
       " 'flavors': 477,\n",
       " 'ranking': 478,\n",
       " 'bookmark': 479,\n",
       " 'dominance': 480,\n",
       " 'resignation': 481,\n",
       " 'rich': 482,\n",
       " 'five': 483,\n",
       " 'great': 484,\n",
       " 'first': 485,\n",
       " 'weekday': 486,\n",
       " 'delivered': 487,\n",
       " 'most': 488,\n",
       " 'although': 489,\n",
       " 'battle': 490,\n",
       " 'saturday': 491,\n",
       " 'both': 492,\n",
       " 'courteous': 493,\n",
       " 'said': 494,\n",
       " 'things': 495,\n",
       " 'recommendations': 496,\n",
       " 'champion': 497,\n",
       " 'with': 498,\n",
       " 'no1': 499,\n",
       " 'ivan': 500,\n",
       " 'sloane': 501,\n",
       " 'surprised': 502,\n",
       " '(and': 503,\n",
       " 'wasnt': 504,\n",
       " 'my': 505,\n",
       " 'sunnyvale': 506,\n",
       " 'moment': 507,\n",
       " 'group': 508,\n",
       " 'chance': 509,\n",
       " 'currently': 510,\n",
       " 'sliders': 511,\n",
       " 'quality': 512,\n",
       " 'drinks': 513,\n",
       " 'themselves': 514,\n",
       " 'once': 515,\n",
       " 'began': 516,\n",
       " 'run-in': 517,\n",
       " 'burgers': 518,\n",
       " 'no83': 519,\n",
       " 'match': 520,\n",
       " 'stage': 521,\n",
       " 'new': 522,\n",
       " 'modric': 523,\n",
       " 'like': 524,\n",
       " 'players': 525,\n",
       " 'burger': 526,\n",
       " 'australia': 527,\n",
       " 'hours': 528,\n",
       " 'than': 529,\n",
       " 'tie-break': 530,\n",
       " 'games': 531,\n",
       " 'reached': 532,\n",
       " 'french': 533,\n",
       " 'want': 534,\n",
       " 'williams': 535}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2idx를 이용, 모든 지문을 수치로 전환하기\n",
    "def encode_paragraph(paragraph):\n",
    "    words = paragraph.split(\" \")\n",
    "    encoded = []\n",
    "    for word in words:\n",
    "        encoded.append([word2idx[word]])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['enc_paragraph'] = df.paragraph.apply(encode_paragraph) # ★ 데이터프레임에 함수 일괄적용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류항목(food or sport)도 수치로 변경 -> 원 핫 인코딩 이용\n",
    "def encode_category(category):\n",
    "    if category == 'food':\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]\n",
    "df['enc_category'] = df.category.apply(encode_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1, 0]\n",
       "1    [1, 0]\n",
       "2    [1, 0]\n",
       "3    [1, 0]\n",
       "4    [1, 0]\n",
       "Name: enc_category, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['enc_category'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 실습에 이용할 RNN은 Dynamic RNN으로 입력값의 다양한 길이를 고려해서 결과값 출력 -> 단어의 수를 알 수 있게 지문별 단어수 미리 계산\n",
    "def word_cnt(paragraph):\n",
    "    return len(paragraph.split(\" \"))\n",
    "\n",
    "df['seq_length'] = df.paragraph.apply(word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     53\n",
       "1     19\n",
       "2     42\n",
       "3     43\n",
       "4     82\n",
       "5     24\n",
       "6     50\n",
       "7     43\n",
       "8     49\n",
       "9     82\n",
       "10    65\n",
       "11    88\n",
       "12    91\n",
       "13    71\n",
       "14    70\n",
       "15    30\n",
       "16    35\n",
       "17    30\n",
       "18    63\n",
       "19    65\n",
       "Name: seq_length, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seq_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "# RNN은 항상 같은 길이의 입력 시퀀스를 받아야 함 -> 패딩 필요 \n",
    "# 패딩이 RNN 계산에 영향을 끼치지 않도록 Dynamic RNN은 패딩 이전의 입력 시퀀스의 실제 길이를 파라미터로 받아 상태값 계산 시 패딩을 제외\n",
    "\n",
    "# 최고로 긴 지문의 단어 수를 구한 후 모든 지문에 패딩을 집어 넣어 최고로 긴 지문과 동일한 길이를 갖도록 만들기\n",
    "max_word_cnt = 0\n",
    "for row in df['paragraph']:\n",
    "    if len(row.split(' ')) > max_word_cnt:\n",
    "        max_word_cnt = len(row.split(' '))\n",
    "\n",
    "print(max_word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    91\n",
       "Name: seq_length, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seq_length'].loc[df['seq_length']==max(df['seq_length'])] # 위와 동일한 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['seq_length']) #동일한 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    91\n",
       "Name: seq_length, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['seq_length'][df['seq_length'].index[df['seq_length'] == max(df['seq_length'])]] #동일한 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 지문이 같은 길이를 갖도록 지문마다 패딩 추가\n",
    "def sequence_padding(enc_paragraph):\n",
    "    seq_length = len(enc_paragraph)\n",
    "    \n",
    "    for i in range(seq_length, max_word_cnt):\n",
    "        enc_paragraph.append([-1]) # max_word_cnt - seq_length 만큼 -1을 추가\n",
    "    \n",
    "    return enc_paragraph\n",
    "\n",
    "df['enc_paragraph'] = df.enc_paragraph.apply(sequence_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[56],\n",
       " [435],\n",
       " [106],\n",
       " [492],\n",
       " [533],\n",
       " [203],\n",
       " [74],\n",
       " [272],\n",
       " [105],\n",
       " [333],\n",
       " [435],\n",
       " [68],\n",
       " [243],\n",
       " [272],\n",
       " [533],\n",
       " [203],\n",
       " [276],\n",
       " [279],\n",
       " [416],\n",
       " [324],\n",
       " [529],\n",
       " [198],\n",
       " [10],\n",
       " [321],\n",
       " [484],\n",
       " [376],\n",
       " [232],\n",
       " [380],\n",
       " [210],\n",
       " [336],\n",
       " [435],\n",
       " [344],\n",
       " [74],\n",
       " [233],\n",
       " [534],\n",
       " [232],\n",
       " [412],\n",
       " [179],\n",
       " [307],\n",
       " [220],\n",
       " [345],\n",
       " [387],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1],\n",
       " [-1]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['enc_paragraph'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 - input: enc_paragraph , target : enc_category, second parameter of Dynamic RNN : seq_length\n",
    "enc_paragraph = np.array(df.enc_paragraph.tolist()) # 20 x 91 을 tolist로 보내면 20 x 91 x 1이 됨\n",
    "enc_category = np.array(df.enc_category.tolist())\n",
    "seq_length = np.array(df.seq_length.tolist())\n",
    "train_X = enc_paragraph\n",
    "train_Y = enc_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 91, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력값의 텐서확인\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#출력값의 텐서 확인\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/lstm_model_overview.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 구현\n",
    "\n",
    "#모델이미지\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/lstm_model_overview.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 문백 벡터(contextualized vector) 생성 단계\n",
    "    (i) 단어를 인덱스로 변환\n",
    "    (ii) 인덱스를 임베딩으로 변환, 임베딩은 학습과정을 통해 단어 유사도를 포함하게 되어 문맥 벡터를 생성하는데 도움을 줌\n",
    "    (iii) LSTM에 임베딩된 시퀀스를 입력해서 최종 상태값을 출력 -> 이 최종 상태값이 문맥 벡터\n",
    "\n",
    "2. 주제 분류 단계\n",
    "    (i) 문맥 벡터를 덴즈 레이어에 입력\n",
    "    (ii) 덴즈 레이어의 출력값을 노드가 2개인 덴즈 레이어에 입력\n",
    "    (iii) 노드가 2개인 덴즈 레이어의 출력값을 소프트맥스에 입력시켜서 예측값 구현\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "#하이퍼파라미터 세팅\n",
    "learning_rate = 0.001\n",
    "n_epochs = 300\n",
    "\n",
    "#입력값 정의\n",
    "X = tf.placeholder(tf.float32, [None, max_word_cnt, 1]) #X_train이 20 x 91 x 1 이므로 \n",
    "#출력값정의\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "#워드 임베딩 레이어\n",
    "embedding = tf.layers.dense(X,5) #5차원 벡터의 임베딩 출력\n",
    "#LSTM 셀\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64) #64차원 벡터의 상태값 출력\n",
    "\n",
    "#출력값과 상태값 저장하기\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding, dtype = tf.float32, sequence_length = seq_length)\n",
    "\n",
    "#상태값은 문백벡터로 사용되며 dense layer 입력값이 됨\n",
    "dense_layer = tf.layers.dense(state.h, 32) #첫번째 덴즈는 32개의 노드를 가짐\n",
    "#logit은 (food, sports)를 원 핫 인코딩으로 구분하기 위해 2차원 벡터로 구성\n",
    "logits = tf.layers.dense(dense_layer, 2) #두번째 덴즈 레이어는 2개의 노드를 가짐 -> 소프트맥스의 입력값이 됨\n",
    "\n",
    "#손실함수\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits = logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#adam 옵티마이저\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 91, 1), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 91, 5), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 64) dtype=float32>)\n",
      "Tensor(\"dense_1/BiasAdd:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(embedding)\n",
    "print(state)\n",
    "print(dense_layer)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "loss: 0.72268546\n",
      " acc: 0.5\n",
      "epoch: 50\n",
      "loss: 0.46723717\n",
      " acc: 0.8\n",
      "epoch: 100\n",
      "loss: 0.377342\n",
      " acc: 0.85\n",
      "epoch: 150\n",
      "loss: 0.1492677\n",
      " acc: 0.95\n",
      "epoch: 200\n",
      "loss: 0.0369424\n",
      " acc: 1.0\n",
      "epoch: 250\n",
      "loss: 0.006650818\n",
      " acc: 1.0\n",
      "epoch: 300\n",
      "loss: 0.0022783908\n",
      " acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "#학습하기\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "        sess.run(optimizer, feed_dict = {X : train_X, y : train_Y})\n",
    "        train_loss = sess.run(loss, feed_dict = {X : train_X, y : train_Y})\n",
    "        if epoch ==1 or epoch % 50 == 0:\n",
    "            preds = tf.nn.softmax(logits)\n",
    "            correct_prediction = tf.equal(tf.argmax(preds,1), tf.argmax(y,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            cur_acc = accuracy.eval({X : train_X, y: train_Y})\n",
    "            print(\"epoch: \"+ str(epoch) + \"\\nloss: \" + str(train_loss) + \"\\n acc: \" + str(cur_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
