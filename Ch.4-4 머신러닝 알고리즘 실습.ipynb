{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.3)\n",
      "(1.1, 0.8)\n",
      "(2.2, 2.8)\n",
      "(3.3, 3.2)\n",
      "(4.4, 4.2)\n",
      "(5.6, 6.0)\n",
      "(6.7, 7.4)\n",
      "(7.8, 9.3)\n",
      "(8.9, 9.0)\n",
      "(10.0, 10.3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8. 선형회귀\n",
    "    (1) [이론]\n",
    "        - 경사하강법 : 회귀모델을 구현할 때 최초 회귀 계수를 임의의 값으로 설정한 후 경사하강법을 반보적으로 실행해 최소의 평균제곱오차를 가지는 회귀 계수(d)를 구함\n",
    "            - 이를 공식으로 표현하면 d:= d - a *∂f(d)/∂d , f(d)는 평균제곱오차 \n",
    "\"\"\"\n",
    "# (2) [실습] 선형회귀 - 케라스로 구현\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#데이터 만들기\n",
    "X = np.linspace(0,10,10) #0부터 10까지 총 10개의 X값\n",
    "\n",
    "# np.arange(start, end, step=간격, dtype = 자료형) -> start~end-1 사이의 값을 간격만큼 띄어 배열로 반환\n",
    "# np.linspace(start, end, num =개수, retsetp =False, dtype = 자료형) -> start~end사이의 값을 (동일한 간격을 지닌) 개수만큼 반환, retstep = True일 시 ([list], step)의 튜플로 반환\n",
    "\n",
    "Y = X + np.random.randn(*X.shape) #Y값은 X에 임의의 수를 더한 값, randn() 에서 괄호값은 생성할 난수 개수, (10,)에서 10을 언패킹\n",
    "\n",
    "#데이터 조회\n",
    "for x, y in zip(X,Y):\n",
    "    print((round(x,1), round(y,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial w is :1.6978716\n"
     ]
    }
   ],
   "source": [
    "#선형회귀 모델 만들기\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=1, units=1, activation='linear', use_bias = False)) #input_dim은 입력값의 차원, units는 y값의 차원, use_bias = False는 intercept 포함 안한다는 말\n",
    "\n",
    "#모델 학습 방법 정의\n",
    "sgd = optimizers.SGD(lr=0.01) # SGD는 경사하강법, lr은 학습률\n",
    "model.compile(optimizer='sgd', loss='mse') #MSE를 줄이는 방법으로 학습\n",
    "\n",
    "#학습 전 최초로 설정된 w값 조회\n",
    "weights = model.layers[0].get_weights() #임의의 가중치 부여\n",
    "w = weights[0][0][0]\n",
    "print('initial w is :' + str(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.9268\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 400us/step - loss: 0.9268\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 300us/step - loss: 0.9268\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 400us/step - loss: 0.9268\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 400us/step - loss: 0.9268\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 400us/step - loss: 0.9268\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 200us/step - loss: 0.9268\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 300us/step - loss: 0.9268\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 300us/step - loss: 0.9268\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 200us/step - loss: 0.9268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f82831b4a8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#선형 회귀 모델 학습\n",
    "model.fit(X,Y, batch_size=10, epochs=10, verbose=1) #batch_size는 한번에 처리할 데이터개수, epochs는 반복학습 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained w is : 1.0070312\n"
     ]
    }
   ],
   "source": [
    "#학습된 기울기 조회\n",
    "weights = model.layers[0].get_weights()\n",
    "w = weights[0][0][0]\n",
    "\n",
    "print('trained w is : ' + str(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 회귀선 시각화\n",
    "plt.plot(X, Y, label='data')\n",
    "plt.plot(X, w*X, label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.3379 - binary_accuracy: 1.0000\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.3360 - binary_accuracy: 1.0000\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3342 - binary_accuracy: 1.0000\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.3324 - binary_accuracy: 1.0000\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.3306 - binary_accuracy: 1.0000\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3288 - binary_accuracy: 1.0000\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3271 - binary_accuracy: 1.0000\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.3254 - binary_accuracy: 1.0000\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 167us/step - loss: 0.3236 - binary_accuracy: 1.0000\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3219 - binary_accuracy: 1.0000\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3203 - binary_accuracy: 1.0000\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3186 - binary_accuracy: 1.0000\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3169 - binary_accuracy: 1.0000\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3153 - binary_accuracy: 1.0000\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3137 - binary_accuracy: 1.0000\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3121 - binary_accuracy: 1.0000\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.3105 - binary_accuracy: 1.0000\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3089 - binary_accuracy: 1.0000\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3074 - binary_accuracy: 1.0000\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3058 - binary_accuracy: 1.0000\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.3043 - binary_accuracy: 1.0000\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.3028 - binary_accuracy: 1.0000\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 499us/step - loss: 0.3013 - binary_accuracy: 1.0000\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2998 - binary_accuracy: 1.0000\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 501us/step - loss: 0.2983 - binary_accuracy: 1.0000\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2968 - binary_accuracy: 1.0000\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2954 - binary_accuracy: 1.0000\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 499us/step - loss: 0.2940 - binary_accuracy: 1.0000\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2925 - binary_accuracy: 1.0000\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2911 - binary_accuracy: 1.0000\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 332us/step - loss: 0.2897 - binary_accuracy: 1.0000\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2883 - binary_accuracy: 1.0000\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2870 - binary_accuracy: 1.0000\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2856 - binary_accuracy: 1.0000\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2843 - binary_accuracy: 1.0000\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2829 - binary_accuracy: 1.0000\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2816 - binary_accuracy: 1.0000\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2803 - binary_accuracy: 1.0000\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2790 - binary_accuracy: 1.0000\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2777 - binary_accuracy: 1.0000\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 501us/step - loss: 0.2764 - binary_accuracy: 1.0000\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2751 - binary_accuracy: 1.0000\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2739 - binary_accuracy: 1.0000\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2726 - binary_accuracy: 1.0000\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2714 - binary_accuracy: 1.0000\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2702 - binary_accuracy: 1.0000\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2690 - binary_accuracy: 1.0000\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2678 - binary_accuracy: 1.0000\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2666 - binary_accuracy: 1.0000\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2654 - binary_accuracy: 1.0000\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2642 - binary_accuracy: 1.0000\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2631 - binary_accuracy: 1.0000\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2619 - binary_accuracy: 1.0000\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2608 - binary_accuracy: 1.0000\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2596 - binary_accuracy: 1.0000\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2585 - binary_accuracy: 1.0000\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2574 - binary_accuracy: 1.0000\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2563 - binary_accuracy: 1.0000\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2552 - binary_accuracy: 1.0000\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2541 - binary_accuracy: 1.0000\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2530 - binary_accuracy: 1.0000\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2519 - binary_accuracy: 1.0000\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2509 - binary_accuracy: 1.0000\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2498 - binary_accuracy: 1.0000\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2488 - binary_accuracy: 1.0000\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2477 - binary_accuracy: 1.0000\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.2467 - binary_accuracy: 1.0000\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2457 - binary_accuracy: 1.0000\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2447 - binary_accuracy: 1.0000\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2437 - binary_accuracy: 1.0000\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2427 - binary_accuracy: 1.0000\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2417 - binary_accuracy: 1.0000\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2407 - binary_accuracy: 1.0000\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 499us/step - loss: 0.2397 - binary_accuracy: 1.0000\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2388 - binary_accuracy: 1.0000\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2378 - binary_accuracy: 1.0000\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2368 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 499us/step - loss: 0.2359 - binary_accuracy: 1.0000\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2350 - binary_accuracy: 1.0000\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2340 - binary_accuracy: 1.0000\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2331 - binary_accuracy: 1.0000\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2322 - binary_accuracy: 1.0000\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2313 - binary_accuracy: 1.0000\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2304 - binary_accuracy: 1.0000\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2295 - binary_accuracy: 1.0000\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2286 - binary_accuracy: 1.0000\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2277 - binary_accuracy: 1.0000\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2268 - binary_accuracy: 1.0000\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2260 - binary_accuracy: 1.0000\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2251 - binary_accuracy: 1.0000\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2243 - binary_accuracy: 1.0000\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2234 - binary_accuracy: 1.0000\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2226 - binary_accuracy: 1.0000\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2217 - binary_accuracy: 1.0000\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2209 - binary_accuracy: 1.0000\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2201 - binary_accuracy: 1.0000\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2192 - binary_accuracy: 1.0000\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2184 - binary_accuracy: 1.0000\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2176 - binary_accuracy: 1.0000\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2168 - binary_accuracy: 1.0000\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2160 - binary_accuracy: 1.0000\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2152 - binary_accuracy: 1.0000\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2145 - binary_accuracy: 1.0000\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2137 - binary_accuracy: 1.0000\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 0s 999us/step - loss: 0.2129 - binary_accuracy: 1.0000\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2121 - binary_accuracy: 1.0000\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2114 - binary_accuracy: 1.0000\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2106 - binary_accuracy: 1.0000\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2099 - binary_accuracy: 1.0000\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2091 - binary_accuracy: 1.0000\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.2084 - binary_accuracy: 1.0000\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2076 - binary_accuracy: 1.0000\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2069 - binary_accuracy: 1.0000\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.2062 - binary_accuracy: 1.0000\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2055 - binary_accuracy: 1.0000\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2047 - binary_accuracy: 1.0000\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.2040 - binary_accuracy: 1.0000\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2033 - binary_accuracy: 1.0000\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2026 - binary_accuracy: 1.0000\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 167us/step - loss: 0.2019 - binary_accuracy: 1.0000\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2012 - binary_accuracy: 1.0000\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.2005 - binary_accuracy: 1.0000\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1998 - binary_accuracy: 1.0000\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1992 - binary_accuracy: 1.0000\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.1985 - binary_accuracy: 1.0000\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1978 - binary_accuracy: 1.0000\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1972 - binary_accuracy: 1.0000\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1965 - binary_accuracy: 1.0000\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1958 - binary_accuracy: 1.0000\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1952 - binary_accuracy: 1.0000\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1945 - binary_accuracy: 1.0000\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.1939 - binary_accuracy: 1.0000\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1933 - binary_accuracy: 1.0000\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 0s 497us/step - loss: 0.1926 - binary_accuracy: 1.0000\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 0s 834us/step - loss: 0.1920 - binary_accuracy: 1.0000\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.1914 - binary_accuracy: 1.0000\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1907 - binary_accuracy: 1.0000\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1901 - binary_accuracy: 1.0000\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1895 - binary_accuracy: 1.0000\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1889 - binary_accuracy: 1.0000\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.1883 - binary_accuracy: 1.0000\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1877 - binary_accuracy: 1.0000\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1871 - binary_accuracy: 1.0000\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1865 - binary_accuracy: 1.0000\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1859 - binary_accuracy: 1.0000\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1853 - binary_accuracy: 1.0000\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1847 - binary_accuracy: 1.0000\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1841 - binary_accuracy: 1.0000\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 0s 499us/step - loss: 0.1836 - binary_accuracy: 1.0000\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1830 - binary_accuracy: 1.0000\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1824 - binary_accuracy: 1.0000\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1818 - binary_accuracy: 1.0000\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1813 - binary_accuracy: 1.0000\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1807 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1802 - binary_accuracy: 1.0000\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1796 - binary_accuracy: 1.0000\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1791 - binary_accuracy: 1.0000\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1785 - binary_accuracy: 1.0000\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1780 - binary_accuracy: 1.0000\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1774 - binary_accuracy: 1.0000\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1769 - binary_accuracy: 1.0000\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 0s 167us/step - loss: 0.1764 - binary_accuracy: 1.0000\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1758 - binary_accuracy: 1.0000\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1753 - binary_accuracy: 1.0000\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1748 - binary_accuracy: 1.0000\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1742 - binary_accuracy: 1.0000\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1737 - binary_accuracy: 1.0000\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1732 - binary_accuracy: 1.0000\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.1727 - binary_accuracy: 1.0000\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1722 - binary_accuracy: 1.0000\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1717 - binary_accuracy: 1.0000\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1712 - binary_accuracy: 1.0000\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1707 - binary_accuracy: 1.0000\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1702 - binary_accuracy: 1.0000\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1697 - binary_accuracy: 1.0000\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1692 - binary_accuracy: 1.0000\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1687 - binary_accuracy: 1.0000\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1682 - binary_accuracy: 1.0000\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1677 - binary_accuracy: 1.0000\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1673 - binary_accuracy: 1.0000\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1668 - binary_accuracy: 1.0000\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1663 - binary_accuracy: 1.0000\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.1658 - binary_accuracy: 1.0000\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1654 - binary_accuracy: 1.0000\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1649 - binary_accuracy: 1.0000\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1644 - binary_accuracy: 1.0000\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1640 - binary_accuracy: 1.0000\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1635 - binary_accuracy: 1.0000\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 0s 667us/step - loss: 0.1630 - binary_accuracy: 1.0000\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1626 - binary_accuracy: 1.0000\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1621 - binary_accuracy: 1.0000\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1617 - binary_accuracy: 1.0000\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1612 - binary_accuracy: 1.0000\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1608 - binary_accuracy: 1.0000\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1604 - binary_accuracy: 1.0000\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1599 - binary_accuracy: 1.0000\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1595 - binary_accuracy: 1.0000\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1590 - binary_accuracy: 1.0000\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1586 - binary_accuracy: 1.0000\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1582 - binary_accuracy: 1.0000\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1578 - binary_accuracy: 1.0000\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1573 - binary_accuracy: 1.0000\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1569 - binary_accuracy: 1.0000\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1565 - binary_accuracy: 1.0000\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1561 - binary_accuracy: 1.0000\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1557 - binary_accuracy: 1.0000\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1552 - binary_accuracy: 1.0000\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.1548 - binary_accuracy: 1.0000\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1544 - binary_accuracy: 1.0000\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1540 - binary_accuracy: 1.0000\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1536 - binary_accuracy: 1.0000\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1532 - binary_accuracy: 1.0000\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1528 - binary_accuracy: 1.0000\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1524 - binary_accuracy: 1.0000\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1520 - binary_accuracy: 1.0000\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1516 - binary_accuracy: 1.0000\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1512 - binary_accuracy: 1.0000\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.1508 - binary_accuracy: 1.0000\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1504 - binary_accuracy: 1.0000\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 0s 999us/step - loss: 0.1500 - binary_accuracy: 1.0000\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1497 - binary_accuracy: 1.0000\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1493 - binary_accuracy: 1.0000\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1489 - binary_accuracy: 1.0000\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 0s 1000us/step - loss: 0.1485 - binary_accuracy: 1.0000\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1481 - binary_accuracy: 1.0000\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1478 - binary_accuracy: 1.0000\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1474 - binary_accuracy: 1.0000\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.1470 - binary_accuracy: 1.0000\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1467 - binary_accuracy: 1.0000\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1463 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1459 - binary_accuracy: 1.0000\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1456 - binary_accuracy: 1.0000\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1452 - binary_accuracy: 1.0000\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1448 - binary_accuracy: 1.0000\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.1445 - binary_accuracy: 1.0000\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1441 - binary_accuracy: 1.0000\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1438 - binary_accuracy: 1.0000\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1434 - binary_accuracy: 1.0000\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1431 - binary_accuracy: 1.0000\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1427 - binary_accuracy: 1.0000\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1424 - binary_accuracy: 1.0000\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1420 - binary_accuracy: 1.0000\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1417 - binary_accuracy: 1.0000\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1413 - binary_accuracy: 1.0000\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1410 - binary_accuracy: 1.0000\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1406 - binary_accuracy: 1.0000\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1403 - binary_accuracy: 1.0000\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1400 - binary_accuracy: 1.0000\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1396 - binary_accuracy: 1.0000\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1393 - binary_accuracy: 1.0000\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1390 - binary_accuracy: 1.0000\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1386 - binary_accuracy: 1.0000\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1383 - binary_accuracy: 1.0000\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1380 - binary_accuracy: 1.0000\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1377 - binary_accuracy: 1.0000\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1373 - binary_accuracy: 1.0000\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1370 - binary_accuracy: 1.0000\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1367 - binary_accuracy: 1.0000\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1364 - binary_accuracy: 1.0000\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1361 - binary_accuracy: 1.0000\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1357 - binary_accuracy: 1.0000\n",
      "Epoch 262/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1354 - binary_accuracy: 1.0000\n",
      "Epoch 263/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1351 - binary_accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1348 - binary_accuracy: 1.0000\n",
      "Epoch 265/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1345 - binary_accuracy: 1.0000\n",
      "Epoch 266/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1342 - binary_accuracy: 1.0000\n",
      "Epoch 267/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1339 - binary_accuracy: 1.0000\n",
      "Epoch 268/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1336 - binary_accuracy: 1.0000\n",
      "Epoch 269/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1333 - binary_accuracy: 1.0000\n",
      "Epoch 270/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1330 - binary_accuracy: 1.0000\n",
      "Epoch 271/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1326 - binary_accuracy: 1.0000\n",
      "Epoch 272/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1323 - binary_accuracy: 1.0000\n",
      "Epoch 273/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1320 - binary_accuracy: 1.0000\n",
      "Epoch 274/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1318 - binary_accuracy: 1.0000\n",
      "Epoch 275/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1315 - binary_accuracy: 1.0000\n",
      "Epoch 276/300\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1312 - binary_accuracy: 1.0000\n",
      "Epoch 277/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1309 - binary_accuracy: 1.0000\n",
      "Epoch 278/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1306 - binary_accuracy: 1.0000\n",
      "Epoch 279/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1303 - binary_accuracy: 1.0000\n",
      "Epoch 280/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1300 - binary_accuracy: 1.0000\n",
      "Epoch 281/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1297 - binary_accuracy: 1.0000\n",
      "Epoch 282/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1294 - binary_accuracy: 1.0000\n",
      "Epoch 283/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1291 - binary_accuracy: 1.0000\n",
      "Epoch 284/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1288 - binary_accuracy: 1.0000\n",
      "Epoch 285/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1286 - binary_accuracy: 1.0000\n",
      "Epoch 286/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1283 - binary_accuracy: 1.0000\n",
      "Epoch 287/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1280 - binary_accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1277 - binary_accuracy: 1.0000\n",
      "Epoch 289/300\n",
      "6/6 [==============================] - 0s 666us/step - loss: 0.1274 - binary_accuracy: 1.0000\n",
      "Epoch 290/300\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1272 - binary_accuracy: 1.0000\n",
      "Epoch 291/300\n",
      "6/6 [==============================] - 0s 833us/step - loss: 0.1269 - binary_accuracy: 1.0000\n",
      "Epoch 292/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1266 - binary_accuracy: 1.0000\n",
      "Epoch 293/300\n",
      "6/6 [==============================] - 0s 334us/step - loss: 0.1263 - binary_accuracy: 1.0000\n",
      "Epoch 294/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1261 - binary_accuracy: 1.0000\n",
      "Epoch 295/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1258 - binary_accuracy: 1.0000\n",
      "Epoch 296/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1255 - binary_accuracy: 1.0000\n",
      "Epoch 297/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1253 - binary_accuracy: 1.0000\n",
      "Epoch 298/300\n",
      "6/6 [==============================] - 0s 499us/step - loss: 0.1250 - binary_accuracy: 1.0000\n",
      "Epoch 299/300\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.1247 - binary_accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "6/6 [==============================] - 0s 500us/step - loss: 0.1245 - binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05981579],\n",
       "       [0.11178291],\n",
       "       [0.1993265 ],\n",
       "       [0.8427933 ],\n",
       "       [0.89879894],\n",
       "       [0.9371671 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "9. 로지스틱 회귀\n",
    "    - (y값이 0 혹은 1로 이루어진 데이터를 추정한) 선형 회귀를 입력으로 받아 특정 레이블로 분류하는 모델\n",
    "    \n",
    "    (1)[이론] 로지스틱 회귀\n",
    "        - wx + b 의 선형회귀를 시그모이드 함수로 입력 -> 시그모이드 값이 0.5 이하일 경우 거짓, 0.5보다 클 경우 참을 반납\n",
    "        - 단일 입력, 다중입력, 다중분류 로지스틱 회귀(=소프트맥스)가 존재\n",
    "        \n",
    "        (i) 로지스틱 회귀 학습\n",
    "            - 로지스틱 회귀 역시 경사하강법으로 최적이 w를 찾아내지만, 비용함수는 평균제곱오차가 아닌 크로스 엔트로피 이용 -> 비선형성을 지니고 있는 시그모이드 함수 때문\n",
    "            \n",
    "        (ii) 선형 vs 비선형\n",
    "            - 시그모이드 함수 = 1 / (1+e^-y) = 1 / (1+e^-(w1x1 + w2x2 + w3x3...)) 로 비선형 함수\n",
    "            - 함수가 선형일 경우 평균제곱오차 비용함수는 아래쪽으로 볼록하기 때문에 경사하강법으로 변곡점을 찾을 수 있음\n",
    "            - 로지스틱 회귀의 평균제곱오차는 볼록한 모양이 아님 -> 크로스 엔트로피 사용\n",
    "            \n",
    "        (iii) 크로스 엔트로피\n",
    "            - 크로스 엔트로피란 서로 다른 두 확률 분포의 차이, 로지스틱 회귀 관점에서는 모델의 예측값의 확률과 실제값 확률의 차이 -> 차이를 가장 작게하는 w를 구함으로써 최적의 w를 구함\n",
    "            - 크로스 엔트로피 = -∑(p(x)*log{q(x)}) ; p(x)는 실제 데이터의 분포, q(x)는 모델의 예측값 분포\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "#(2) [실습] 단일 입력 로지스틱 회귀\n",
    "# 1개의 입력을 받아 0 또는 1을 출력하는 로지스틱 회귀 모델을 케라스로 구현\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "#로지스틱 회귀 모델 만들기\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=1,units=1)) #입력 1개를 받아 출력 1개를 리턴하는 선형 회귀 레이어 생성\n",
    "model.add(Activation('sigmoid')) #선형 회귀의 출력값을 시그모이드에 연결\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'sgd', metrics=['binary_accuracy']) #크로스 엔트로피를 비용함수로 설정해 경사하강법으로 학습\n",
    "\n",
    "#데이터 생성\n",
    "X = np.array([-2, -1.5, -1, 1.25, 1.62, 2])\n",
    "Y = np.array([0,0,0,1,1,1])\n",
    "\n",
    "#모델학습\n",
    "model.fit(X,Y, epochs = 300, verbose=0) #300번의 반복학습 ; verbose 는 진행상황을 알려줌 0이면 silent, 1이면 progress bar, 2면 one line per epoch\n",
    "\n",
    "#모델의 출력값 확인\n",
    "model.predict([-2,-1.5,-1,1.25,1.62,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 요약\n",
    "model.summary()\n",
    "# dense_3이 선형회귀, activation_3이 시그모이드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3/kernel:0' shape=(1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#레이어에 존재하는 w와 b는 다음과 같이 확인\n",
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.1038953]], dtype=float32), array([-0.01740905], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습을 통해 구현한 w와 b는 get_weights() 함수로 확인\n",
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) [실습] 다중 입려 로지스틱 회귀\n",
    "# AND 연산을 로지스틱 회귀로 구현\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "#로지스틱 회귀 모델 만들기 - sigmoid(w1x1 + w2x2 + b) 형태를 띠는 간단한 로지스틱 회귀 구현\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=2,units=1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='sgd',metrics=['binary_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19605897],\n",
       "       [0.40030167],\n",
       "       [0.35892287],\n",
       "       [0.6051236 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습 데이터 생성\n",
    "X = np.array([(0,0), (0,1),(1,0),(1,1)])\n",
    "Y = np.array([0,0,0,1])\n",
    "\n",
    "#모델 학습\n",
    "model.fit(X,Y, epochs = 500, verbose = 0)\n",
    "\n",
    "#출력\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_4/kernel:0' shape=(2, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.8310681],\n",
       "        [1.0069021]], dtype=float32),\n",
       " array([-1.4111104], dtype=float32)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 7s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# (4) [실습] 소프트맥스(다중분류 로지스틱 회귀)\n",
    "# MNIST 손글씨 숫자 데이터 셋을 사용해 입력된 손글씨 숫자를 0부터 9까지 분류\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#데이터 획득\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data (count, row, column) : (60000, 28, 28)\n",
      "test data (count, row, column) : (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data (count, row, column) : \" + str(X_train.shape))\n",
    "print(\"test data (count, row, column) : \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정규화 -> 입력 값을 0부터 1의 값으로 변경\n",
    "#정규화된 입력값은 경사하강법으로 모델을 학습할 떄 더욱 쉽고 빠르게 w*, b*를 찾게 도와줌\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train target (count) :(60000,)\n",
      "test target (count) :(10000,)\n"
     ]
    }
   ],
   "source": [
    "# y는 손글씨 데이터에 해당하는 숫자\n",
    "print(\"train target (count) :\" + str(y_train.shape))\n",
    "print(\"test target (count) :\" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from train :5\n",
      "sample from test :7\n"
     ]
    }
   ],
   "source": [
    "print(\"sample from train :\" + str(y_train[0])) #샘플 숫자 출력\n",
    "print(\"sample from test :\" + str(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 단순화\n",
    "input_dim = 784 #28 픽셀 x 28 픽셀\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#소프트 맥스\n",
    "# 실제값과의 크로스 엔트로피를 계산하기 위해 실제값을 '원 핫 인코딩'으로 변환\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0]) # 5였던 값이 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#케라스로 소프트맥스 구현\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=input_dim, units=10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 78us/step\n",
      "test accuracy: 0.8902\n"
     ]
    }
   ],
   "source": [
    "#모델 학습\n",
    "model.compile(optimizer = 'sgd', loss='categorical_crossentropy',metrics=['accuracy']) #10개의 클래스로 분류 -> categorical_crossentropy 사용\n",
    "model.fit(X_train, y_train, batch_size=2048, epochs=100, verbose=0)\n",
    "\n",
    "#모델 테스트\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('test accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_5/kernel:0' shape=(784, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
