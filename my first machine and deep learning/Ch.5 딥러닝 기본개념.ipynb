{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 딥러닝(심층신경망;Deep Neural Network, DNN)의 탄생\n",
    "    - 뉴런(노드)이 일렬로 존재하는 구조를 '레이어', '레이어'가 여러가지 이어진 것이 신경망\n",
    "\n",
    "2. 딥러닝과 머신러능의 관계\n",
    "    - 딥러닝 ⊂ 머신러닝\n",
    "\n",
    "3. 딥러닝 이름의 유래\n",
    "\n",
    "4. 딥러닝 탄생 배경\n",
    "    - 퍼셉트론이 시초\n",
    "    \n",
    "5. 퍼셉트론\n",
    "    - 두개의 입력을 하나의 출력으로 바꾸는 모델, 가장 단순한 신경망\n",
    "    - z = w1x1 + w2x2 + b\n",
    "    - AND 와 OR은 하나의 선으로 분류 가능 ((0,0), (0,1), (1,0), (1,1)을 AND, OR에 따라 하나의 선으로 결과값으로 나눈다고 생각) -> 하나의 퍼셉트론이 필요\n",
    "    - 반면 XOR은 두개의 선이 필요 -> 두개의 퍼셉트론이 필요\n",
    "\n",
    "6. 다층 퍼셉트론(Multi Layer Perceptron; MLP)\n",
    "    - 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)이 존재\n",
    "\n",
    "7. 뉴런\n",
    "    - 뉴런(노드)는 퍼셉트론과 유사\n",
    "    - 활성화 함수로 다양한 비선형 함수를 사용 ; 비선형함수를 사용하는 이유는 역전파를 사용한 모델 학습시 활성화 함수가 미분가능해야 하기 때문\n",
    "    \n",
    "8. 딥러닝의 학습\n",
    "    - 목표는 모델에 입력값을 넣었을 때의 출력값이 최대한 정답과 일치하게 하는 것\n",
    "    - 최초 딥러닝 모델의 매개변수를 무작위로 부여한 후, 반복하습을 통해 모델의 출력값을 최대한 정답과 일치하도록 매개변수 조정\n",
    "    \n",
    "    (1) 순전파(forward propagation)\n",
    "        - 딥러닝에 값을 입력해서 출력을 얻는 과정\n",
    "        - 순전파 과정을 거치면 출력값 y hat을 어데 되고 정답과 출력값 차이를 구할 수 있게 됨\n",
    "        - 출력값과 정답의 차이를 구하기 위한 함수를 '손실함수'라고 함\n",
    "    \n",
    "    (2) 손실함수\n",
    "        - 출력값과 정답의 차이를 계산\n",
    "        - 보통 회귀에는 평균제곱오차를, 분류 문제에서는 크로스 엔트로피를 손실함수로 사용\n",
    "        - 매개변수를 조절해서 손실함수의 값을 최저로 만드는 과정을 최적화 과정이라고 하며, 이는 옵티마이저를 통해 이루어짐\n",
    "        - 옵티마이저는 역전파 과정을 수행해서 모델의 매개변수 최적화\n",
    "    \n",
    "    (3) 최적화\n",
    "        - 대표적인 최적화 방법이 경사하강법 : new 매개변수 = old 매개변수 - 학습률 * dl/dw\n",
    "        - 여러 개의 매개변수가 여러 레이어에 존재할 때 옵티마이저는 여러 레이어에 존재하는 매개변수를 조정하기 위해 역전파 사용\n",
    "    \n",
    "    (4) 역전파(backward propagation)\n",
    "        - 옵티마이저는 손실함수의 값을 최저로 하기 위해 역전파를 사용해 딥러닝 모델의 모든 매개변수를 변경\n",
    "        - 각 매개변수의 미분값은 연쇄법칙을 이용해서 구할 수 있음\n",
    "        \n",
    "    (5) 옵티마이저\n",
    "        (i) 배치 경사하강법\n",
    "            - 딥러닝 모델을 최적화하는 가장 기본적인 방법\n",
    "            - 로컬 미니멈에 멈출 수 있다는 단점\n",
    "        \n",
    "        (ii) SGD(Stochastic gradient descent)\n",
    "            - 모든 데이터를 계산해서 매개변수를 변경하는 배치 경사하강법과 달리 하나의 데이터마다 매개변수를 변경하는 방법\n",
    "            - 학습이 용이, 로컬 미니멈이 많을 땐 SGD가 배치 경사하강법보다 더 나은 최적화 알고리즘\n",
    "            - 배치 경사하강법보다 못한 매개변수로 학습이 완료될 수도 있음\n",
    "        \n",
    "        (iii) 미니 배치\n",
    "            - 배치 경사하강법과 SGD의 절충\n",
    "            - 전체 데이터를 계산해서 매개변수를 변경하는 대신 정해진 양만큼 계산해서 매개변수를 최적화\n",
    "            - 용어 정리\n",
    "                -주기(epoch) : 학습을 위해 전체 데이터를 다 사용했을 때 한 주기, 학습을 시작하기 전 최대주기 설정\n",
    "                -배치 사이즈 : 매개변수를 조정을 위해 한번에 처리하는 데이터의 양\n",
    "                -스텝(step, iteration) : 미니 배치를 사용해 매개변수가 조정되는 순간; 1000개의 데이터가 있고 배치사이즈가 100일 경우 총 10번의 스텝으로 10번의 매개변수 조정이 이루어짐, 10번의 스텝은 1개의 주기가 됨\n",
    "            - 매우 실용적이어서 많은 딥러닝에서 활용\n",
    "        \n",
    "        (iv) 모멘텀\n",
    "            - 로컬 미니멈으로 학습되는 걸 방지하기 위한 방법\n",
    "            - new 매개변수 = old매개변수 + 이동변수, 이동변수 = 모멘텀 - 학습률 * dl/dw, 모멘텀 = 모멘텀조정률 * 이동변수\n",
    "            - 글로벌 미니멈으로 최적화된다는 보장은 없음\n",
    "        \n",
    "        (v) 학습률\n",
    "            - 너무 크면 수렴 X, 너무 작으면 시간이 오래 걸림\n",
    "            - 최초 무작위로 설정된 매개변수 위치에서는 로컬 미니멈에서 멀리있을 확률이 크므로 학습률을 크게 설정해서 학습속도를 증진시키고, 모델이 어느정도 학습되면 매개변수가 로컬 미니멈에 수렴하도록 학습률을 작게 조정\n",
    "            - 중간에 학습률을 조정하는 방식을 decay라고 함\n",
    "            \n",
    "        (vi) Adagrad\n",
    "            - 각 매개변수에 각기 다른 학습률을 적용하고, 빈번히 변화가 찾아오는 가중치는 학습률이 작게 설정되고, 변화가 적은 가중치는 학습률이 높게 설정되는 옵티마이저\n",
    "            - 개발자가 직접 decay를 신경쓰지 않아도 모델이 알아서 가중치별로 학습률 지정\n",
    "            - 자연어 처리에 장점\n",
    "        \n",
    "        (vii) Adam\n",
    "            - Adagrad의 학습률 자율 조정과 모멘텀의 효율적인 매개변수 변경 알고리즘을 조합한 알고리즘\n",
    "            - 최근들어 가장 많이 활용\n",
    "\n",
    "9. 딥러닝의 과대 적합\n",
    "    - 딥러닝은 학습시 매개변수가 만핟는 점과 학습 횟수에 제한이 없다는 점에서 과대적합이 발생할 가능성이 높음\n",
    "    \n",
    "    (1) 드롭아웃\n",
    "        - 매개변수 중 일정량을 학습 중간마다 무작위로 사용하지 않는 방법\n",
    "        - 드롭아웃은 모델에 앙상블 효과를 줌으로써 모델의 분산을 줄임(분산투자랑 같은 원리) -> 과대적합 감소\n",
    "    \n",
    "    (2) 조기 종료\n",
    "        - 데이터를 학습/테스트/검증 데이터로 나누고, 학습 후 검증데이터에 대한 정확도가 떨어지기 시작할 때 학습을 중단함으로써 과대적합 방지, 학습시간 단축\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#10. [실습] 퍼셉트론\n",
    "import tensorflow as tf\n",
    "\n",
    "#데이터 만들기\n",
    "T = 1.0\n",
    "F = 0.0\n",
    "bias = 1.0\n",
    "\n",
    "#AND 데이터\n",
    "def get_AND_data():\n",
    "    X = [\n",
    "    [F, F, bias],\n",
    "    [F, T, bias],\n",
    "    [T, F, bias],\n",
    "    [T, T, bias]\n",
    "    ]\n",
    "    \n",
    "    Y = [\n",
    "        [F],\n",
    "        [F],\n",
    "        [F],\n",
    "        [T]\n",
    "    ]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "#OR 데이터\n",
    "def get_OR_data():\n",
    "    X = [\n",
    "    [F, F, bias],\n",
    "    [F, T, bias],\n",
    "    [T, F, bias],\n",
    "    [T, T, bias]\n",
    "    ]\n",
    "    \n",
    "    Y = [\n",
    "        [F],\n",
    "        [T],\n",
    "        [T],\n",
    "        [T]\n",
    "    ]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "#XOR 데이터\n",
    "def get_XOR_data():\n",
    "    X = [\n",
    "    [F, F, bias],\n",
    "    [F, T, bias],\n",
    "    [T, F, bias],\n",
    "    [T, T, bias]\n",
    "    ]\n",
    "    \n",
    "    Y = [\n",
    "        [F],\n",
    "        [T],\n",
    "        [T],\n",
    "        [F]\n",
    "    ]\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "#AND 데이터 호출\n",
    "X,Y = get_AND_data()\n",
    "\n",
    "#총 3개의 입력과 1개의 출력을 내므로 이와 관련된 가중치 설정\n",
    "W = tf.Variable(tf.random_normal([3,1]))\n",
    "\n",
    "#스텝함수 설정\n",
    "def step(x):\n",
    "    return tf.to_float(tf.greater(x,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 mse: 0.25\n",
      "epoch: 2 mse: 0.5\n",
      "epoch: 3 mse: 0.25\n",
      "epoch: 4 mse: 0.25\n",
      "epoch: 5 mse: 0.25\n",
      "epoch: 6 mse: 0.5\n",
      "epoch: 7 mse: 0.25\n",
      "epoch: 8 mse: 0.0\n",
      "Testing reulst:\n",
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#손실함수로 평균제곱오차 사용\n",
    "f = tf.matmul(X,W) #matrix multiplication\n",
    "output = step(f)\n",
    "error = tf.subtract(Y,output)\n",
    "mse = tf.reduce_mean(tf.square(error))\n",
    "\n",
    "#퍼셉트론은 미분 불가능한 스텝함수를 사용하므로 경사하강법 대신 w_new = w_old ± input의 형식으로 최적화 진행\n",
    "delta = tf.matmul(X, error, transpose_a = True) # X.T * error\n",
    "train = tf.assign(W, tf.add(W,delta))\n",
    "\n",
    "#테스트 진행\n",
    "init = tf.global_variables_initializer() #텐서플로 변수를 초기화\n",
    "\n",
    "#세션실행\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    err = 1\n",
    "    epoch, max_epochs = 0, 20\n",
    "    while err > 0.0 and epoch < max_epochs:\n",
    "        epoch += 1\n",
    "        err = sess.run(mse)\n",
    "        sess.run(train)\n",
    "        print('epoch:' , epoch, 'mse:', err)\n",
    "    \n",
    "    print('Testing reulst:')\n",
    "    print (sess.run([output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_1:0' shape=(3, 1) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. [실습] 다층 퍼셉트론으로 XOR 구현하기\n",
    "X = tf.placeholder(tf.float32, shape=[4,2])\n",
    "Y = tf.placeholder(tf.float32, shape=[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#두개의 입력값을 받는 두개의 뉴런 생성\n",
    "W1 = tf.Variable(tf.random_uniform([2,2]))\n",
    "#편향값 설정\n",
    "B1 = tf.Variable(tf.zeros([2])) #두개를 출력하므로 편향값은 2개\n",
    "Z = tf.sigmoid(tf.matmul(X,W1) + B1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_uniform([2,1])) #Z값을 입력값으로 받는 두번째 뉴런\n",
    "B2 = tf.Variable(tf.zeros([1])) #편향값은 1개\n",
    "\n",
    "Y_hat = tf.sigmoid(tf.matmul(Z,W2) + B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
      "Epoch : 0\n",
      "Output : [[0.63064003]\n",
      " [0.67122334]\n",
      " [0.66952217]\n",
      " [0.700558  ]]\n",
      "Epoch : 5000\n",
      "Output : [[0.19128592]\n",
      " [0.6389202 ]\n",
      " [0.62011945]\n",
      " [0.57589126]]\n",
      "Epoch : 10000\n",
      "Output : [[0.05996781]\n",
      " [0.9422153 ]\n",
      " [0.94227356]\n",
      " [0.071202  ]]\n",
      "Epoch : 15000\n",
      "Output : [[0.02828424]\n",
      " [0.9768461 ]\n",
      " [0.97686255]\n",
      " [0.02622345]]\n",
      "Final Output : [[0.01819682]\n",
      " [0.9858673 ]\n",
      " [0.9858753 ]\n",
      " [0.0155218 ]]\n"
     ]
    }
   ],
   "source": [
    "#손실함수로 크로스 엔트로피 사용\n",
    "loss = tf.reduce_mean(-1*((Y*tf.log(Y_hat)) + (1-Y)*tf.log(1.0-Y_hat)))\n",
    "\n",
    "#경사하강법으로 모델의 매개변수를 최적화\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "#학습데이터 설정\n",
    "train_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "train_Y = [[0],[1],[1],[0]]\n",
    "\n",
    "#학습 진행\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"train data: \" + str(train_X))\n",
    "    #2만번의 반복학습 진행\n",
    "    for i in range(20000):\n",
    "        sess.run(train_step, feed_dict={X: train_X, Y:train_Y})\n",
    "        if i % 5000 == 0:\n",
    "            print('Epoch :', i)\n",
    "            print('Output :', sess.run(Y_hat, feed_dict = {X:train_X, Y:train_Y}))\n",
    "    print('Final Output :', sess.run(Y_hat, feed_dict = {X:train_X, Y:train_Y}))\n",
    "    \n",
    "#학습 결과 [0,0]과 [1,1]은 0에 아주 가까운 값을, [0,1]과 [1,0]은 0에 아주 가까운 값을 반납"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#12. [실습] 다층 퍼셉트론으로 손글씨 숫자 분류하기\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#다층 퍼셉트론 구조\n",
    "from IPython.display import Image\n",
    "Image(url= \"https://raw.githubusercontent.com/captainchargers/deeplearning/master/img/dropout.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터와 검증데이터 분리\n",
    "x_val = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(50000,784) # 1차원 배열형태로 변경\n",
    "x_val = x_val.reshape(10000,784)\n",
    "x_test = x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 정규화\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale\n",
    "\n",
    "#분류 문제이므로 손실함수로 크로스 엔트로피 사용, y값은 원핫인코딩으로 변경\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값(784) ->1층(784->256) -> 2층(256->128) -> 3층(128 -> 10) ->출력층(10), 소프트맥스로 0~9까지 반납\n",
    "# Adam 옵티마이저 사용\n",
    "# 2층에서 드롭아웃 10%적용\n",
    "\n",
    "#입력값과 출력값 정의\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#드롭아웃 적용변수 정의\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#다층 퍼셉트론 구현\n",
    "def mlp(x):\n",
    "    #히든 레이어 1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    \n",
    "    #히든 레이어 2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1,w2) + b2)\n",
    "    \n",
    "    #드롭아웃 적용\n",
    "    h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "    \n",
    "    #히든 레이어 3\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits = tf.matmul(h2_drop, w3) + b3\n",
    "    \n",
    "    return logits\n",
    "\n",
    "logits=mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0, train_acc: 0.367, val acc: 0.39\n",
      "epoch :1, train_acc: 0.4875, val acc: 0.5073\n",
      "epoch :2, train_acc: 0.55846, val acc: 0.5784\n",
      "epoch :3, train_acc: 0.60256, val acc: 0.6149\n",
      "epoch :4, train_acc: 0.66914, val acc: 0.6874\n",
      "epoch :5, train_acc: 0.7141, val acc: 0.7278\n",
      "epoch :6, train_acc: 0.74842, val acc: 0.7639\n",
      "epoch :7, train_acc: 0.776, val acc: 0.79\n",
      "epoch :8, train_acc: 0.79762, val acc: 0.8106\n",
      "epoch :9, train_acc: 0.81516, val acc: 0.8286\n",
      "epoch :10, train_acc: 0.82992, val acc: 0.8405\n",
      "epoch :11, train_acc: 0.84272, val acc: 0.8517\n",
      "epoch :12, train_acc: 0.8533, val acc: 0.8602\n",
      "epoch :13, train_acc: 0.86286, val acc: 0.8684\n",
      "epoch :14, train_acc: 0.87128, val acc: 0.8758\n",
      "epoch :15, train_acc: 0.87808, val acc: 0.8816\n",
      "epoch :16, train_acc: 0.8831, val acc: 0.8861\n",
      "epoch :17, train_acc: 0.89004, val acc: 0.8915\n",
      "epoch :18, train_acc: 0.89406, val acc: 0.8945\n",
      "epoch :19, train_acc: 0.8994, val acc: 0.8985\n",
      "epoch :20, train_acc: 0.90326, val acc: 0.9022\n",
      "epoch :21, train_acc: 0.90666, val acc: 0.9058\n",
      "epoch :22, train_acc: 0.91016, val acc: 0.9072\n",
      "epoch :23, train_acc: 0.91202, val acc: 0.9083\n",
      "epoch :24, train_acc: 0.91566, val acc: 0.9107\n",
      "epoch :25, train_acc: 0.91804, val acc: 0.9137\n",
      "epoch :26, train_acc: 0.92012, val acc: 0.9172\n",
      "epoch :27, train_acc: 0.9229, val acc: 0.9175\n",
      "epoch :28, train_acc: 0.92504, val acc: 0.9193\n",
      "epoch :29, train_acc: 0.92746, val acc: 0.9215\n",
      "epoch :30, train_acc: 0.92898, val acc: 0.9233\n",
      "epoch :31, train_acc: 0.93108, val acc: 0.9235\n",
      "epoch :32, train_acc: 0.93236, val acc: 0.926\n",
      "epoch :33, train_acc: 0.93422, val acc: 0.9268\n",
      "epoch :34, train_acc: 0.93646, val acc: 0.9274\n",
      "epoch :35, train_acc: 0.93706, val acc: 0.9272\n",
      "overfitting warnig: 0\n",
      "epoch :36, train_acc: 0.9391, val acc: 0.9294\n",
      "epoch :37, train_acc: 0.94018, val acc: 0.9303\n",
      "epoch :38, train_acc: 0.94106, val acc: 0.9311\n",
      "epoch :39, train_acc: 0.94258, val acc: 0.9321\n",
      "epoch :40, train_acc: 0.94354, val acc: 0.9327\n",
      "epoch :41, train_acc: 0.94492, val acc: 0.9339\n",
      "epoch :42, train_acc: 0.94608, val acc: 0.9345\n",
      "epoch :43, train_acc: 0.94708, val acc: 0.9349\n",
      "epoch :44, train_acc: 0.94788, val acc: 0.9359\n",
      "epoch :45, train_acc: 0.94862, val acc: 0.9362\n",
      "epoch :46, train_acc: 0.94978, val acc: 0.9363\n",
      "epoch :47, train_acc: 0.95052, val acc: 0.9359\n",
      "overfitting warnig: 0\n",
      "epoch :48, train_acc: 0.95134, val acc: 0.9374\n",
      "epoch :49, train_acc: 0.95274, val acc: 0.9374\n",
      "epoch :50, train_acc: 0.95348, val acc: 0.9391\n",
      "epoch :51, train_acc: 0.95374, val acc: 0.9386\n",
      "overfitting warnig: 0\n",
      "epoch :52, train_acc: 0.95458, val acc: 0.9384\n",
      "overfitting warnig: 1\n",
      "epoch :53, train_acc: 0.95508, val acc: 0.9385\n",
      "overfitting warnig: 2\n",
      "epoch :54, train_acc: 0.95536, val acc: 0.9379\n",
      "overfitting warnig: 3\n",
      "epoch :55, train_acc: 0.95638, val acc: 0.939\n",
      "overfitting warnig: 4\n",
      "epoch :56, train_acc: 0.9568, val acc: 0.9382\n",
      "early stopped on56\n"
     ]
    }
   ],
   "source": [
    "# logit과 실제값의 크로스 엔트로피를 손실함수로 사용\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2( logits = logits, labels = y))\n",
    "\n",
    "# Adam 옵티마이저를 사용해 모델 최적화\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(loss_op)\n",
    "\n",
    "# 매 주기마다 검증 정확도를 측정 후 검증 적확도가 5번 연속으로 최고 검증 정확도보다 높지 않을 경우 조기 종료 수행하도록 설정\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver() #텐서플로 모델 저장 및 로드\n",
    "\n",
    "#조기 종료가 일어나지 않을 경우 최고 300번까지 반복 학습\n",
    "epoch_cnt = 300\n",
    "batch_size = 1000\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "# 5번 연속 검증 정확도가 최고 정확도보다 높지 않을 경우 조기 종료\n",
    "earlystop_threshold = 5\n",
    "earlystock_cnt = 0\n",
    "\n",
    "#학습 진행\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0\n",
    "        start = 0; end = batch_size\n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], feed_dict = {x: x_train[start:end], y: y_train[start: end], keep_prob :0.9}) # keep_prob 을 0.9로 설정함으로써 10% 드롭아웃\n",
    "            \n",
    "            start += batch_size; end += batch_size\n",
    "            \n",
    "            #학습 손실값 계산\n",
    "            avg_loss += loss / iteration\n",
    "        \n",
    "        #모델검증\n",
    "        preds = tf.nn.softmax(logits)\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y,1))\n",
    "        \n",
    "        #정확도 계산\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float')) #tf.cast()는 소수점을 없앰\n",
    "        \n",
    "        #학습 정확도 계산\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train, keep_prob: 1.0}) # a.eval()은 session()이 실행된 상황에서 sess.run(a)의 속기 표현\n",
    "        \n",
    "        #검증 정확도 계산\n",
    "        cur_val_acc = accuracy.eval({x:x_val, y: y_val, keep_prob: 1.0})\n",
    "        \n",
    "        #검증 데이터에 대한 손실값 계산\n",
    "        cur_val_loss = loss_op.eval({x:x_val, y: y_val, keep_prob : 1.0})\n",
    "        \n",
    "        print(\"epoch :\" + str(epoch) + \", train_acc: \" + str(cur_train_acc) + \", val acc: \" + str(cur_val_acc))\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print('early stopped on' + str(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    print('overfitting warnig: ' + str(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else: \n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            #검증정확도가 가장 높은 모델 저장\n",
    "            save_path = saver.save(sess, 'model/model.ckpt')\n",
    "        prev_train_acc = cur_train_acc\n",
    "        \n",
    "            \n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "[Test accuracy] : 0.9337\n"
     ]
    }
   ],
   "source": [
    "#검증 정확도가 가장 높은 모델로 테스트 진행\n",
    "with tf.Session() as sess:\n",
    "    #검증 결과가 가장 높았던 모델 불러오기\n",
    "    saver.restore(sess, 'model/model.ckpt')\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    #정확도 계산\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    print('[Test accuracy] :', accuracy.eval({x: x_test, y: y_test, keep_prob : 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
