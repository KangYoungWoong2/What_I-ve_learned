{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"(Vanila) RNN(Recurrent Neural Network, 순환신경망) : RNN 중 가장 단순한 모델\n",
    "\n",
    "현재의 상태값 h_t = f(h_t-1, x_t) = tanh(W_hh * h_t-1 + W_hx * x_t)\n",
    "현재의 출력 y_t = W_hy * h_t\n",
    "\n",
    "RNN applications : https://github.com/TensorFlowKR/awesome_tensorflow_implementations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" RNN in tesorflow \n",
    "\n",
    "1. 상태값을 내는 셀을 만든다 : cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) , num_units 는 출력(상태값)의 개수, 셀은 자신이 사용하는 모델 등에 따라 BasicLSTMCell 등으로 교체 가능\n",
    "2. 셀을 구동해서 출력을 받는다 : output, _state = tf.nn.dynamic_rnn(cell, x_data, dtype = tf.float32) , 이때 output이 h_t가 됨\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#RNN의 입력과 출력\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 원핫인코딩 'hello'\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "# 이때 입력값 x의 shape는 (1,1,4)가 되도록 하며 출력은 (1,1,n)이 되도록 hidden size를 통해 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.]]], dtype=float32)\n",
      "array([[[0.00273436, 0.52379775]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('one_cell5') as scope: #one_variable이란 variable_scope를 한번 사용하면 동일한 작업 불가능\n",
    "    # One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cells = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "    \n",
    "    x_data = np.array([[h]], dtype=np.float32) # x_data = [[[1,0,0,0]]]\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cells, x_data, dtype=tf.float32)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "array([[[ 0.5437193 ,  0.46096808],\n",
      "        [-0.40864253,  0.5796649 ],\n",
      "        [ 0.28415743, -0.6981258 ],\n",
      "        [-0.6302551 , -0.58918905],\n",
      "        [ 0.5594544 ,  0.1453845 ]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input의 shape와 output의 (a,b,c) c가 각각 input_dimension, hidden_size였다면 b는 입력 데이터의 길이(sequence length)를 의미\n",
    "with tf.variable_scope('one_cell6') as scope:\n",
    "# One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cells = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "    x_data = np.array([[h,e,l,l,o]], dtype=np.float32) \n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cells, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "array([[[ 0.26099256, -0.23209658],\n",
      "        [-0.19653676,  0.319594  ],\n",
      "        [ 0.2630962 , -0.19141088],\n",
      "        [ 0.19510952, -0.73888487],\n",
      "        [-0.51928085, -0.00628737]],\n",
      "\n",
      "       [[-0.13814099,  0.61244816],\n",
      "        [ 0.14025319, -0.00230351],\n",
      "        [ 0.1967387 , -0.60706186],\n",
      "        [ 0.3502664 , -0.70193034],\n",
      "        [-0.58453816, -0.09241634]],\n",
      "\n",
      "       [[-0.13814099,  0.61244816],\n",
      "        [ 0.13513002,  0.3441358 ],\n",
      "        [-0.71843004,  0.7203274 ],\n",
      "        [ 0.38907456,  0.47649246],\n",
      "        [-0.09080583, -0.46673477]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#input shape와 output의 (a,b,c)에서 a는 batch_size -> 학습할 데이터의 양\n",
    "with tf.variable_scope('one_cell9') as scope:\n",
    "# One cell RNN input_dim (4) -> output_dim (2)\n",
    "    hidden_size = 2\n",
    "    cells = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "\n",
    "    x_data = np.array([[h,e,l,l,o], [e,l,l,h,o],[e,h,o,l,l]], dtype=np.float32) # x_data = [[[1,0,0,0]]]\n",
    "    pp.pprint(x_data)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cells, x_data, dtype=tf.float32)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teach RNN 'hihello'\n",
    "# h ->i , i -> h, h ->e, ... , l -> o 처럼 다음글자를 예측할 수 있도록 학습\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o'] #문자열\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # 입력 : hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # 출력 : ihello\n",
    "\n",
    "\n",
    "with tf.variable_scope('one_cell12') as scope: #다시 시작해도 변경해주어야 함\n",
    "    num_classes = 5 # 문자열의 개수\n",
    "    input_dim = 5  # 입력 문자열의 개수\n",
    "    hidden_size = 5  # 출력 문자열의 개수\n",
    "    batch_size = 1   # one sentence\n",
    "    sequence_length = 6  # |ihello| == 6\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    X = tf.placeholder(\n",
    "        tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
    "    Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label: ihello\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True) #셀만들기\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32) #초기값 배정\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32) # 셀 구동\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6820976\n"
     ]
    }
   ],
   "source": [
    "# Sequence RNN에서 사용하는 cost function : Sequnce loss 예시\n",
    "y_data = tf.constant([[1,1,1,]])\n",
    "\n",
    "prediction = tf.constant([[[0.2,0.7],[0.3,0.5],[0.7,0.2]]], dtype = tf.float32)\n",
    "\n",
    "weight = tf.constant([[1,1,1]], dtype = tf.float32)\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets = y_data, weights = weight)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Loss: ', sess.run(sequence_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.6035075 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "1 loss: 1.5366507 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "2 loss: 1.4777532 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "3 loss: 1.4193172 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "4 loss: 1.3533754 prediction:  [[3 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  llllll\n",
      "5 loss: 1.2810712 prediction:  [[2 3 3 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  elllll\n",
      "6 loss: 1.210656 prediction:  [[2 3 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  elelll\n",
      "7 loss: 1.153573 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "8 loss: 1.1059629 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "9 loss: 1.0641327 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "10 loss: 1.0300307 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "11 loss: 1.0013375 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "12 loss: 0.97566813 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "13 loss: 0.95304 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "14 loss: 0.93450546 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "15 loss: 0.920396 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "16 loss: 0.909023 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "17 loss: 0.89894503 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "18 loss: 0.8898584 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "19 loss: 0.8822416 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "20 loss: 0.8754697 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "21 loss: 0.8682184 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "22 loss: 0.8605868 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "23 loss: 0.85338974 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "24 loss: 0.8464625 prediction:  [[2 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ehelll\n",
      "25 loss: 0.83915156 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "26 loss: 0.83175564 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "27 loss: 0.82457256 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "28 loss: 0.8173598 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "29 loss: 0.8100416 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "30 loss: 0.8028826 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "31 loss: 0.7958924 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "32 loss: 0.78891134 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "33 loss: 0.7822954 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "34 loss: 0.77612007 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "35 loss: 0.77023125 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "36 loss: 0.76485384 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "37 loss: 0.7600011 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "38 loss: 0.755422 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "39 loss: 0.75123703 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "40 loss: 0.7472148 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "41 loss: 0.74313426 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "42 loss: 0.7389827 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "43 loss: 0.73439014 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "44 loss: 0.7292428 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "45 loss: 0.72339773 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "46 loss: 0.7168812 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "47 loss: 0.71033543 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "48 loss: 0.70438135 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n",
      "49 loss: 0.6995424 prediction:  [[1 0 2 3 3 3]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\tPrediction str:  ihelll\n"
     ]
    }
   ],
   "source": [
    "#학습\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\tPrediction str: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN with long sequence\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))  # index -> char : 중복을 없애 리스트로 받기\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> index로 해서 딕셔너리로 받기\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "learning_rate = 0.1\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "\n",
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "\n",
    "with tf.variable_scope('lstm3') as score:\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, _states = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.2847593 Prediction: yyyy u u  yyyuu\n",
      "1 loss: 2.1808865 Prediction: yyyy u     uuuu\n",
      "2 loss: 2.061739 Prediction: yyyy u      uuu\n",
      "3 loss: 1.9443986 Prediction: yyyyou aa  yyou\n",
      "4 loss: 1.8236507 Prediction: yyyyouoaa yyyou\n",
      "5 loss: 1.7534765 Prediction: yyyy   aa  yyou\n",
      "6 loss: 1.700007 Prediction: yyyy   aa  yyou\n",
      "7 loss: 1.653459 Prediction: yyyy   aa   y u\n",
      "8 loss: 1.5864625 Prediction: yyyy   aa   yo \n",
      "9 loss: 1.5305682 Prediction: yyyy   aa   yo \n",
      "10 loss: 1.4798074 Prediction: yfyy   aan  you\n",
      "11 loss: 1.4330196 Prediction: yfyyou aan  you\n",
      "12 loss: 1.3850362 Prediction: yf you aant you\n",
      "13 loss: 1.3440365 Prediction: yf you wantyyou\n",
      "14 loss: 1.3105631 Prediction: yf you want you\n",
      "15 loss: 1.2805848 Prediction: yf you want you\n",
      "16 loss: 1.2549009 Prediction: yf youuwant you\n",
      "17 loss: 1.2394685 Prediction: yf youuwant you\n",
      "18 loss: 1.226584 Prediction: yf youuwant you\n",
      "19 loss: 1.2068338 Prediction: yf youuwant you\n",
      "20 loss: 1.1841968 Prediction: if youwwant you\n",
      "21 loss: 1.1631689 Prediction: if you want you\n",
      "22 loss: 1.1780946 Prediction: if you want you\n",
      "23 loss: 1.1486003 Prediction: if youuwant you\n",
      "24 loss: 1.1490817 Prediction: if youuwant you\n",
      "25 loss: 1.149375 Prediction: if youuwant you\n",
      "26 loss: 1.145809 Prediction: if youuwant you\n",
      "27 loss: 1.1381007 Prediction: if youuwant you\n",
      "28 loss: 1.1275172 Prediction: if you want you\n",
      "29 loss: 1.1162851 Prediction: if you want you\n",
      "30 loss: 1.1073333 Prediction: if you want you\n",
      "31 loss: 1.1026198 Prediction: if you want you\n",
      "32 loss: 1.1015724 Prediction: if you want you\n",
      "33 loss: 1.0967118 Prediction: if you want you\n",
      "34 loss: 1.0903145 Prediction: if you want you\n",
      "35 loss: 1.0862877 Prediction: if you want you\n",
      "36 loss: 1.0846013 Prediction: if youwwant you\n",
      "37 loss: 1.0834895 Prediction: if youwwant you\n",
      "38 loss: 1.0818021 Prediction: if youwwant you\n",
      "39 loss: 1.0791461 Prediction: if youwwant you\n",
      "40 loss: 1.0757874 Prediction: if youwwant you\n",
      "41 loss: 1.072684 Prediction: if youwwant you\n",
      "42 loss: 1.0708625 Prediction: if youwwant you\n",
      "43 loss: 1.0700219 Prediction: if you want you\n",
      "44 loss: 1.0684257 Prediction: if you want you\n",
      "45 loss: 1.0660654 Prediction: if you want you\n",
      "46 loss: 1.0642611 Prediction: if you want you\n",
      "47 loss: 1.0627784 Prediction: if you want you\n",
      "48 loss: 1.061451 Prediction: if you want you\n",
      "49 loss: 1.0600177 Prediction: if you want you\n"
     ]
    }
   ],
   "source": [
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN with long sequence (다중 레이어와 softmax를 결합)\n",
    "# Deep 한 RNN을 만들기 위해 RNN을 쌓음 \n",
    "# cell = tf.rnn.BasicLSTMCell(hidden_size, state_is_tuple = True)\n",
    "# cell = tf.rnn.MultiRNNCell([cell] * 2, state_is_tuple = True) #셀을 2층으로 쌓음\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {w: i for i, w in enumerate(char_set)}\n",
    "\n",
    "data_dim = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "sequence_length = 10  # Any arbitrary number\n",
    "learning_rate = 0.1\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x = [char_dic[c] for c in x_str]  # x str to index\n",
    "    y = [char_dic[c] for c in y_str]  # y str to index\n",
    "\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "\n",
    "batch_size = len(dataX)\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "# One-hot encoding\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(X_one_hot)  # check out the shape\n",
    "\n",
    "\n",
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "def lstm_cell():\n",
    "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    return cell\n",
    "\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n",
    "\n",
    "# outputs: unfolding size x hidden size, state = hidden size\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
    "\n",
    "# FC layer(소프트맥스 사용)\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "\n",
    "# All weights are 1 (equal weights)\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(500):\n",
    "    _, l, results = sess.run(\n",
    "        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
    "    for j, result in enumerate(results):\n",
    "        index = np.argmax(result, axis=1)\n",
    "        print(i, j, ''.join([char_set[t] for t in index]), l)\n",
    "\n",
    "# Let's print the last char of each result to check it works\n",
    "results = sess.run(outputs, feed_dict={X: dataX})\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([char_set[t] for t in index]), end='')\n",
    "    else:\n",
    "        print(char_set[index[-1]], end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
