{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4671665\n",
      "200 0.4494747\n",
      "400 0.43163657\n",
      "600 0.41543177\n",
      "800 0.40055165\n",
      "1000 0.38676262\n",
      "1200 0.3738906\n",
      "1400 0.36180663\n",
      "1600 0.3504158\n",
      "1800 0.3396463\n",
      "2000 0.3294426\n",
      "2200 0.31975967\n",
      "2400 0.31056002\n",
      "2600 0.30181113\n",
      "2800 0.29348418\n",
      "3000 0.28555313\n",
      "3200 0.27799413\n",
      "3400 0.27078506\n",
      "3600 0.2639055\n",
      "3800 0.2573361\n",
      "4000 0.251059\n",
      "4200 0.2450576\n",
      "4400 0.23931623\n",
      "4600 0.23382014\n",
      "4800 0.22855544\n",
      "5000 0.22350943\n",
      "5200 0.21867\n",
      "5400 0.21402574\n",
      "5600 0.20956612\n",
      "5800 0.2052813\n",
      "6000 0.20116186\n",
      "6200 0.19719917\n",
      "6400 0.19338499\n",
      "6600 0.18971176\n",
      "6800 0.18617219\n",
      "7000 0.18275948\n",
      "7200 0.17946748\n",
      "7400 0.17629011\n",
      "7600 0.17322175\n",
      "7800 0.17025721\n",
      "8000 0.16739155\n",
      "8200 0.16461995\n",
      "8400 0.1619382\n",
      "8600 0.15934218\n",
      "8800 0.15682787\n",
      "9000 0.15439166\n",
      "9200 0.1520301\n",
      "9400 0.14973988\n",
      "9600 0.14751787\n",
      "9800 0.14536124\n",
      "10000 0.14326717\n",
      "\n",
      "Hypothesis:  [[0.02816371]\n",
      " [0.15519828]\n",
      " [0.29229802]\n",
      " [0.7871852 ]\n",
      " [0.9431742 ]\n",
      " [0.98138404]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "''' logistic regression '''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 기본적으로 regression + classification(sigmoid 함수를 적용, 0.5보다 크면 1, 0.5보다 작으면 0)\n",
    "\n",
    "# cost 함수는 로컬 미니멈이 있는 MSE 대신 cross-entropy\n",
    "# C(H(x), y) = -log(H(x)) if y = 1 or -log(1-H(x)) if y = 0 = -ylog(H(x)) - (1-y)log(1-H(x))\n",
    "# cross entropy를 이용한 경사하강법 이용\n",
    "\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]] # 이진 분류 logistic regression이기 때문에 Y데이터는 0 또는 1로 이루어져있어야 함\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight') # x 데이터가 n x 2고 하나의 값을 내야 하므로 w는 2 x 1\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b) # H(x) = 1 / 1 + e^(-XW + b) , hypothsis = tf.div(1. , 1. + tf.exp(-tf.matmul(X,W)))\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)* tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32) # 1과 0으로 변환하는데 있어 기준점이 되는 값 0.5 설정\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32)) # tf.equal(A,B) 은 두개의 값이 같으면 True, 다르면 False 반납 -> 이를 reduced_mean으로 평균을 냄\n",
    "\n",
    "#학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # 정확성\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.443914\n",
      "200 0.63929695\n",
      "400 0.51892287\n",
      "600 0.41869053\n",
      "800 0.32393542\n",
      "1000 0.25158808\n",
      "1200 0.22632937\n",
      "1400 0.20577154\n",
      "1600 0.18854411\n",
      "1800 0.17389613\n",
      "2000 0.1612928\n",
      "--------------\n",
      "[[8.2720080e-03 9.9172217e-01 5.7714356e-06]] [1]\n",
      "--------------\n",
      "[[0.83200955 0.16245934 0.00553115]] [0]\n",
      "--------------\n",
      "[[1.2663491e-08 3.2328328e-04 9.9967670e-01]] [2]\n",
      "--------------\n",
      "[[8.2720080e-03 9.9172217e-01 5.7714356e-06]\n",
      " [8.3200955e-01 1.6245934e-01 5.5311495e-03]\n",
      " [1.2663491e-08 3.2328328e-04 9.9967670e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Softmax Regression : 0과 1 사이의 확률로 나타냄\n",
    "텐서플로우에서는 hypothesis = tf.nn.softmax(tf.matmul(X,W) + b) ; 이때 softmax의 인수를 logit 이라고도 함\n",
    "\n",
    "이때 loss function은 역시 cross-entropy: -(1/m)∑Y_i * log(W*X_i + b) \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)\n",
    "\n",
    "    print('--------------')\n",
    "    # 테스트 & 원핫코딩\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1))) # argmax는 softmax 중 몇번째에 있는 elemtent가 가장 높은 확률을 가지고 있는지 반환\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
