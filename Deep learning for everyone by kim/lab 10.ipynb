{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-4cb1ba5661c5>:12: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\1\\anaconda3\\envs\\virtualtensor3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\1\\anaconda3\\envs\\virtualtensor3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\1\\anaconda3\\envs\\virtualtensor3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\1\\anaconda3\\envs\\virtualtensor3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\1\\anaconda3\\envs\\virtualtensor3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\1\\anaconda3\\envs\\virtualtensor3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Epoch: 0001, Cost: 5.745170969\n",
      "Epoch: 0002, Cost: 1.780056698\n",
      "Epoch: 0003, Cost: 1.122778631\n",
      "Epoch: 0004, Cost: 0.872012247\n",
      "Epoch: 0005, Cost: 0.738203178\n",
      "Epoch: 0006, Cost: 0.654728883\n",
      "Epoch: 0007, Cost: 0.596023602\n",
      "Epoch: 0008, Cost: 0.552216816\n",
      "Epoch: 0009, Cost: 0.518254962\n",
      "Epoch: 0010, Cost: 0.491113193\n",
      "Epoch: 0011, Cost: 0.468347534\n",
      "Epoch: 0012, Cost: 0.449374341\n",
      "Epoch: 0013, Cost: 0.432675650\n",
      "Epoch: 0014, Cost: 0.418828148\n",
      "Epoch: 0015, Cost: 0.406128927\n",
      "Epoch: 0016, Cost: 0.394982937\n",
      "Epoch: 0017, Cost: 0.385870409\n",
      "Epoch: 0018, Cost: 0.376135575\n",
      "Epoch: 0019, Cost: 0.368269366\n",
      "Epoch: 0020, Cost: 0.361209760\n",
      "Epoch: 0021, Cost: 0.354798129\n",
      "Epoch: 0022, Cost: 0.348525116\n",
      "Epoch: 0023, Cost: 0.342752712\n",
      "Epoch: 0024, Cost: 0.337285900\n",
      "Epoch: 0025, Cost: 0.332443590\n",
      "Epoch: 0026, Cost: 0.327556521\n",
      "Epoch: 0027, Cost: 0.324047218\n",
      "Epoch: 0028, Cost: 0.319670888\n",
      "Epoch: 0029, Cost: 0.315536195\n",
      "Epoch: 0030, Cost: 0.312257751\n",
      "Epoch: 0031, Cost: 0.308550797\n",
      "Epoch: 0032, Cost: 0.305987596\n",
      "Epoch: 0033, Cost: 0.302624444\n",
      "Epoch: 0034, Cost: 0.299895888\n",
      "Epoch: 0035, Cost: 0.297245862\n",
      "Epoch: 0036, Cost: 0.294490159\n",
      "Epoch: 0037, Cost: 0.292061196\n",
      "Epoch: 0038, Cost: 0.290009226\n",
      "Epoch: 0039, Cost: 0.287633513\n",
      "Epoch: 0040, Cost: 0.285644488\n",
      "Epoch: 0041, Cost: 0.283856589\n",
      "Epoch: 0042, Cost: 0.281824805\n",
      "Epoch: 0043, Cost: 0.280098960\n",
      "Epoch: 0044, Cost: 0.278386728\n",
      "Epoch: 0045, Cost: 0.276589548\n",
      "Epoch: 0046, Cost: 0.275093693\n",
      "Epoch: 0047, Cost: 0.273444037\n",
      "Epoch: 0048, Cost: 0.271918671\n",
      "Epoch: 0049, Cost: 0.270640430\n",
      "Epoch: 0050, Cost: 0.269054365\n",
      "Learning Finished!\n",
      "Accuracy: 0.9194\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" 1. 소프트맥스(신경망없이 바로 Y^도출) : Accuracy = 91%\"\"\"\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(\n",
    "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-2-83efd9b8d993>:38: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 cost = 166.203090900\n",
      "Epoch: 0002 cost = 41.003340419\n",
      "Epoch: 0003 cost = 25.674757200\n",
      "Epoch: 0004 cost = 17.894962591\n",
      "Epoch: 0005 cost = 12.984837702\n",
      "Epoch: 0006 cost = 9.579409165\n",
      "Epoch: 0007 cost = 7.145340601\n",
      "Epoch: 0008 cost = 5.466962259\n",
      "Epoch: 0009 cost = 4.074605340\n",
      "Epoch: 0010 cost = 3.054815814\n",
      "Epoch: 0011 cost = 2.341831223\n",
      "Epoch: 0012 cost = 1.758112607\n",
      "Epoch: 0013 cost = 1.219694212\n",
      "Epoch: 0014 cost = 0.999854006\n",
      "Epoch: 0015 cost = 0.829347909\n",
      "Learning Finished!\n",
      "Accuracy: 0.9463\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 3단 심층 신경망 적용 : Accuracy = 94% \"\"\"\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 3단 심층신경망 -> gradient vanishing 등의 문제로 sigmoid보다 relu가 훨씬 효율적\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.302030732\n",
      "Epoch: 0002 cost = 0.120232361\n",
      "Epoch: 0003 cost = 0.077709040\n",
      "Epoch: 0004 cost = 0.055697821\n",
      "Epoch: 0005 cost = 0.041212294\n",
      "Epoch: 0006 cost = 0.031716757\n",
      "Epoch: 0007 cost = 0.025887363\n",
      "Epoch: 0008 cost = 0.020467954\n",
      "Epoch: 0009 cost = 0.016417527\n",
      "Epoch: 0010 cost = 0.018362814\n",
      "Epoch: 0011 cost = 0.012933539\n",
      "Epoch: 0012 cost = 0.009682765\n",
      "Epoch: 0013 cost = 0.010402646\n",
      "Epoch: 0014 cost = 0.009965402\n",
      "Epoch: 0015 cost = 0.011139172\n",
      "Learning Finished!\n",
      "Accuracy: 0.9754\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 3단 심층신경망 + Xavier Initialization; Xavier Initialization은 초기값을 부여하는 방식 중 하나로 초기값을 적절하게 부여함으로써 초반의 cost를 낮추고 학습의 효율의 높음\n",
    "Accuracy = 97.5%\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer()) #샤비에 initialization 적용\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 0.454717166\n",
      "Epoch: 0002 cost = 0.170607041\n",
      "Epoch: 0003 cost = 0.129739843\n",
      "Epoch: 0004 cost = 0.108306012\n",
      "Epoch: 0005 cost = 0.092694056\n",
      "Epoch: 0006 cost = 0.080613817\n",
      "Epoch: 0007 cost = 0.075137204\n",
      "Epoch: 0008 cost = 0.067820075\n",
      "Epoch: 0009 cost = 0.064660248\n",
      "Epoch: 0010 cost = 0.056379866\n",
      "Epoch: 0011 cost = 0.055330522\n",
      "Epoch: 0012 cost = 0.050578009\n",
      "Epoch: 0013 cost = 0.052254358\n",
      "Epoch: 0014 cost = 0.044697922\n",
      "Epoch: 0015 cost = 0.046500929\n",
      "Learning Finished!\n",
      "Accuracy: 0.9804\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 5단심층신경망 + DropOut + Xavier initialization  ; 단순히 3단에서 5단으로 신경망을 늘리는 것은 과대적합의 문제를 야기하므로 오히려 정확성을 떨어뜨림 -> Dropout 기법 적용\n",
    "Accuracy = 98%\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "w1 = tf.get_variable(\"w1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "w2 = tf.get_variable(\"w2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, w2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "w3 = tf.get_variable(\"w3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, w3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "w4 = tf.get_variable(\"w4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, w4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "w5 = tf.get_variable(\"w5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, w5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7} #학습시 drop out rate = 30%\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1})) #테스트 시에는 반드시 drop out rate = 0 이어야 함\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
